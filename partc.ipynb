{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1abac8e6-b3df-43b7-959d-bbee6170937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman(FCA_slope, share_bridge_authors) = -0.083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1642862/718632442.py:294: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot(data, labels=[\"Top-FCA pairs\", \"Bottom-FCA pairs\"], showmeans=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done Part C!\n",
      "- Saved: novelty_AB_20250816_124944/link_fca_bes_per_pair.csv\n",
      "- Saved: novelty_AB_20250816_124944/impact_top_vs_bottom.csv\n",
      "- Plots: novelty_AB_20250816_124944/plots\n",
      "- Output folder: novelty_AB_20250816_124944\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Part C — Novelty mining from Parts A (FCA) and B (BES)\n",
    "\n",
    "Outputs (saved under novelty_AB_<timestamp>/):\n",
    "  - link_fca_bes_per_pair.csv\n",
    "  - impact_top_vs_bottom.csv\n",
    "  - plots/ (scatter & box/violin)\n",
    "  - README.txt\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import glob\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------- Config ---------------------------------\n",
    "YEARS = list(range(2018, 2026))             # 2018–2025 inclusive\n",
    "ARTICLES_DIR_TEMPLATE = \"articles_{year}_new\"\n",
    "ARTICLES_FILE = \"all_articles_enhanced.jsonl\"\n",
    "\n",
    "# How many pairs to analyze on each side of FCA (top and bottom)\n",
    "TOP_N_PAIRS = 50\n",
    "\n",
    "# Quantile to define \"bridge authors\" by BES\n",
    "BRIDGE_QUANTILE = 0.90\n",
    "\n",
    "# Minimum #years a field pair must appear (should match Part A)\n",
    "MIN_YEARS_PER_PAIR = 3\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "def find_latest_dir(prefix: str):\n",
    "    \"\"\"Find latest output dir by prefix (e.g., 'field_convergence_A_').\"\"\"\n",
    "    candidates = [d for d in os.listdir(\".\") if d.startswith(prefix) and os.path.isdir(d)]\n",
    "    if not candidates:\n",
    "        return None\n",
    "    # sort by timestamp suffix if exists\n",
    "    candidates.sort(reverse=True)\n",
    "    return candidates[0]\n",
    "\n",
    "def safe_pair(a, b):\n",
    "    \"\"\"Canonical ordered pair key.\"\"\"\n",
    "    return tuple(sorted((a, b)))\n",
    "\n",
    "def ensure_outdir():\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = f\"novelty_AB_{ts}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(out_dir, \"plots\"), exist_ok=True)\n",
    "    return out_dir\n",
    "\n",
    "def load_partA_latest():\n",
    "    a_dir = find_latest_dir(\"field_convergence_A_\")\n",
    "    if a_dir is None:\n",
    "        raise FileNotFoundError(\"Could not find 'field_convergence_A_*' output dir.\")\n",
    "    fca_path = os.path.join(a_dir, \"fca_summary.csv\")\n",
    "    per_year_weights_path = os.path.join(a_dir, \"per_year_field_weights.csv\")\n",
    "    if not os.path.exists(fca_path):\n",
    "        raise FileNotFoundError(f\"Missing {fca_path}\")\n",
    "    df_fca = pd.read_csv(fca_path)\n",
    "    # canonical pair\n",
    "    df_fca[\"pair\"] = list(map(lambda xy: safe_pair(xy[0], xy[1]), zip(df_fca[\"field_i\"], df_fca[\"field_j\"])))\n",
    "    # keep only pairs with enough years\n",
    "    df_fca = df_fca[df_fca[\"years_covered\"] >= MIN_YEARS_PER_PAIR].copy()\n",
    "    return a_dir, df_fca, per_year_weights_path\n",
    "\n",
    "def load_partB_latest():\n",
    "    b_dir = find_latest_dir(\"bridge_emergence_B_\")\n",
    "    if b_dir is None:\n",
    "        raise FileNotFoundError(\"Could not find 'bridge_emergence_B_*' output dir.\")\n",
    "    bes_path = os.path.join(b_dir, \"bes_summary.csv\")\n",
    "    per_year_part_path = os.path.join(b_dir, \"per_year_author_participation.csv\")\n",
    "    if not os.path.exists(bes_path):\n",
    "        raise FileNotFoundError(f\"Missing {bes_path}\")\n",
    "    df_bes = pd.read_csv(bes_path)\n",
    "    return b_dir, df_bes, per_year_part_path\n",
    "\n",
    "def pick_target_pairs(df_fca: pd.DataFrame, top_n=50):\n",
    "    \"\"\"Pick top-N and bottom-N FCA pairs.\"\"\"\n",
    "    df_sorted = df_fca.sort_values(\"FCA_slope\", ascending=False)\n",
    "    top = df_sorted.head(top_n).copy()\n",
    "    bottom = df_sorted.tail(top_n).copy()\n",
    "    top[\"set\"] = \"top\"\n",
    "    bottom[\"set\"] = \"bottom\"\n",
    "    both = pd.concat([top, bottom], axis=0, ignore_index=True)\n",
    "    pairs = set(both[\"pair\"].tolist())\n",
    "    return both, pairs\n",
    "\n",
    "def read_article_items_by_year(year):\n",
    "    folder = ARTICLES_DIR_TEMPLATE.format(year=year)\n",
    "    path = os.path.join(folder, ARTICLES_FILE)\n",
    "    if not os.path.exists(path):\n",
    "        return []\n",
    "    return path  # we return path and stream line-by-line later\n",
    "\n",
    "def iter_author_pair_hits(year, target_pairs):\n",
    "    \"\"\"\n",
    "    Yields tuples for authors whose paper contains any of the target field pairs:\n",
    "      (year, paper_id, pair(tuple), author_id, citation_per_reference_ratio)\n",
    "    \"\"\"\n",
    "    path = read_article_items_by_year(year)\n",
    "    if not path:\n",
    "        return\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            fields = obj.get(\"fields\") or []\n",
    "            fields = [str(x).strip() for x in fields if x and str(x).strip()]\n",
    "            fields = sorted(set(fields))\n",
    "            if len(fields) < 2:\n",
    "                continue\n",
    "\n",
    "            # generate all field pairs present in this paper\n",
    "            paper_pairs = set(safe_pair(a, b) for a, b in itertools.combinations(fields, 2))\n",
    "            matched_pairs = paper_pairs & target_pairs\n",
    "            if not matched_pairs:\n",
    "                continue\n",
    "\n",
    "            paper_id = obj.get(\"id\")\n",
    "            cpr = obj.get(\"citation_per_reference_ratio\", None)\n",
    "            # fall back\n",
    "            if cpr is None:\n",
    "                ref = obj.get(\"referenced_works_count\") or 1\n",
    "                cited = obj.get(\"cited_by_count\") or 0\n",
    "                cpr = float(cited) / float(ref)\n",
    "\n",
    "            authors = obj.get(\"authors\") or []\n",
    "            author_ids = []\n",
    "            for a in authors:\n",
    "                aid = a.get(\"author_id\")\n",
    "                if aid:\n",
    "                    author_ids.append(aid)\n",
    "            author_ids = list(dict.fromkeys(author_ids))\n",
    "            if not author_ids:\n",
    "                continue\n",
    "\n",
    "            for p in matched_pairs:\n",
    "                for aid in author_ids:\n",
    "                    yield (year, paper_id, p, aid, float(cpr))\n",
    "\n",
    "def bootstrap_diff_mean(a, b, iters=2000, random_state=42):\n",
    "    \"\"\"\n",
    "    Returns (diff_mean, ci_low, ci_high) using percentile bootstrap (95% CI).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    na, nb = len(a), len(b)\n",
    "    if na == 0 or nb == 0:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    diffs = []\n",
    "    for _ in range(iters):\n",
    "        sa = rng.choice(a, size=na, replace=True)\n",
    "        sb = rng.choice(b, size=nb, replace=True)\n",
    "        diffs.append(sa.mean() - sb.mean())\n",
    "    diffs = np.array(diffs)\n",
    "    return float((a.mean() - b.mean())), float(np.percentile(diffs, 2.5)), float(np.percentile(diffs, 97.5))\n",
    "\n",
    "def main():\n",
    "    out_dir = ensure_outdir()\n",
    "    plots_dir = os.path.join(out_dir, \"plots\")\n",
    "\n",
    "    # ---- Load A & B ----\n",
    "    a_dir, df_fca, _ = load_partA_latest()\n",
    "    b_dir, df_bes, _ = load_partB_latest()\n",
    "\n",
    "    # define bridge authors (top quantile by BES_delta)\n",
    "    thr = df_bes[\"BES_delta\"].quantile(BRIDGE_QUANTILE)\n",
    "    bridge_authors = set(df_bes.loc[df_bes[\"BES_delta\"] >= thr, \"author_id\"].astype(str))\n",
    "\n",
    "    # pick target pairs from FCA\n",
    "    df_targets, target_pairs = pick_target_pairs(df_fca, top_n=TOP_N_PAIRS)\n",
    "\n",
    "    # ---- Scan years and accumulate stats ----\n",
    "    pair_author_total = Counter()       # (#author-labeled hits per pair)\n",
    "    pair_author_bridge = Counter()      # (#bridge-author hits per pair)\n",
    "    pair_papers = defaultdict(set)      # paper ids per pair (to aggregate impact once per paper)\n",
    "    pair_paper_cpr = defaultdict(list)  # list of CPR per pair (unique papers)\n",
    "\n",
    "    for year in YEARS:\n",
    "        path = read_article_items_by_year(year)\n",
    "        if not path:\n",
    "            continue\n",
    "        # we will track per (pair, paper) whether added cpr\n",
    "        seen_pair_paper = set()\n",
    "        for (yr, pid, pair, aid, cpr) in iter_author_pair_hits(year, target_pairs):\n",
    "            pair_author_total[pair] += 1\n",
    "            if str(aid) in bridge_authors:\n",
    "                pair_author_bridge[pair] += 1\n",
    "\n",
    "            # impact per paper per pair\n",
    "            key_pp = (pair, pid)\n",
    "            if key_pp not in seen_pair_paper:\n",
    "                seen_pair_paper.add(key_pp)\n",
    "                pair_papers[pair].add(pid)\n",
    "                pair_paper_cpr[pair].append(cpr)\n",
    "\n",
    "    # ---- Build pair-level DataFrame ----\n",
    "    rows = []\n",
    "    for pair in target_pairs:\n",
    "        total = pair_author_total.get(pair, 0)\n",
    "        bridges = pair_author_bridge.get(pair, 0)\n",
    "        share = (bridges / total) if total > 0 else np.nan\n",
    "        n_papers = len(pair_papers.get(pair, set()))\n",
    "        cprs = pair_paper_cpr.get(pair, [])\n",
    "        rows.append({\n",
    "            \"field_i\": pair[0],\n",
    "            \"field_j\": pair[1],\n",
    "            \"pair\": pair,\n",
    "            \"authors_total\": int(total),\n",
    "            \"authors_bridge\": int(bridges),\n",
    "            \"share_bridge_authors\": float(share) if share == share else np.nan,\n",
    "            \"papers_count\": int(n_papers),\n",
    "            \"cpr_mean\": float(np.mean(cprs)) if cprs else np.nan,\n",
    "            \"cpr_median\": float(np.median(cprs)) if cprs else np.nan\n",
    "        })\n",
    "    df_link = pd.DataFrame(rows)\n",
    "\n",
    "    # merge FCA slope\n",
    "    df_link = df_link.merge(\n",
    "        df_fca[[\"pair\", \"FCA_slope\", \"weight_mean\", \"year_first\", \"year_last\"]],\n",
    "        on=\"pair\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # save CSV\n",
    "    link_path = os.path.join(out_dir, \"link_fca_bes_per_pair.csv\")\n",
    "    df_link.sort_values(\"FCA_slope\", ascending=False).to_csv(link_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # ---- Correlation FCA ↔ share of bridge authors (novelty 1) ----\n",
    "    corr = df_link[[\"FCA_slope\", \"share_bridge_authors\"]].corr(method=\"spearman\").iloc[0,1]\n",
    "    # simple print\n",
    "    print(f\"Spearman(FCA_slope, share_bridge_authors) = {corr:.3f}\")\n",
    "\n",
    "    # scatter plot\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.scatter(df_link[\"FCA_slope\"], df_link[\"share_bridge_authors\"])\n",
    "    plt.xlabel(\"FCA_slope (field convergence acceleration)\")\n",
    "    plt.ylabel(\"Share of bridge authors (BES top-quantile)\")\n",
    "    plt.title(\"FCA vs. Bridge-Author Share (per field pair)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, \"scatter_fca_vs_bridge_share.png\"), dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "    # ---- Impact: CPR Top-FCA vs Bottom-FCA (novelty 2) ----\n",
    "    df_top = df_link.sort_values(\"FCA_slope\", ascending=False).head(TOP_N_PAIRS)\n",
    "    df_bot = df_link.sort_values(\"FCA_slope\", ascending=True).head(TOP_N_PAIRS)\n",
    "\n",
    "    cpr_top = []\n",
    "    for p in df_top[\"pair\"]:\n",
    "        cpr_top.extend(pair_paper_cpr.get(p, []))\n",
    "    cpr_bot = []\n",
    "    for p in df_bot[\"pair\"]:\n",
    "        cpr_bot.extend(pair_paper_cpr.get(p, []))\n",
    "\n",
    "    diff, ci_lo, ci_hi = bootstrap_diff_mean(cpr_top, cpr_bot, iters=3000, random_state=7)\n",
    "\n",
    "    impact_rows = [{\n",
    "        \"top_pairs\": TOP_N_PAIRS,\n",
    "        \"bottom_pairs\": TOP_N_PAIRS,\n",
    "        \"n_papers_top\": len(cpr_top),\n",
    "        \"n_papers_bottom\": len(cpr_bot),\n",
    "        \"cpr_mean_top\": float(np.mean(cpr_top)) if cpr_top else np.nan,\n",
    "        \"cpr_mean_bottom\": float(np.mean(cpr_bot)) if cpr_bot else np.nan,\n",
    "        \"mean_diff_top_minus_bottom\": diff,\n",
    "        \"bootstrap_95ci_low\": ci_lo,\n",
    "        \"bootstrap_95ci_high\": ci_hi\n",
    "    }]\n",
    "    df_impact = pd.DataFrame(impact_rows)\n",
    "    impact_path = os.path.join(out_dir, \"impact_top_vs_bottom.csv\")\n",
    "    df_impact.to_csv(impact_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # box/violin\n",
    "    plt.figure(figsize=(7,5))\n",
    "    data = [cpr_top, cpr_bot]\n",
    "    plt.boxplot(data, labels=[\"Top-FCA pairs\", \"Bottom-FCA pairs\"], showmeans=True)\n",
    "    plt.ylabel(\"Citations per Reference (CPR)\")\n",
    "    plt.title(\"Impact: Top vs Bottom FCA pairs\")\n",
    "    plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(plots_dir, \"box_impact_top_vs_bottom.png\"), dpi=140)\n",
    "    plt.close()\n",
    "\n",
    "    # optional violin (if you prefer)\n",
    "    try:\n",
    "        plt.figure(figsize=(7,5))\n",
    "        plt.violinplot(data, showmeans=True, showextrema=True)\n",
    "        plt.xticks([1,2], [\"Top-FCA pairs\", \"Bottom-FCA pairs\"])\n",
    "        plt.ylabel(\"Citations per Reference (CPR)\")\n",
    "        plt.title(\"Impact: Top vs Bottom FCA pairs (Violin)\")\n",
    "        plt.grid(True, axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plots_dir, \"violin_impact_top_vs_bottom.png\"), dpi=140)\n",
    "        plt.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ---- README with summary ----\n",
    "    readme = f\"\"\"Novelty Mining from Parts A & B\n",
    "====================================\n",
    "\n",
    "This folder aggregates cross-evidence between:\n",
    "  - Part A (FCA: field convergence acceleration), and\n",
    "  - Part B (BES: bridge emergence for authors).\n",
    "\n",
    "Key outputs:\n",
    "------------\n",
    "1) link_fca_bes_per_pair.csv\n",
    "   Per field pair:\n",
    "     * FCA_slope\n",
    "     * authors_total, authors_bridge, share_bridge_authors\n",
    "     * papers_count\n",
    "     * CPR statistics (mean/median)\n",
    "     * coverage years\n",
    "\n",
    "2) impact_top_vs_bottom.csv\n",
    "   CPR comparison between Top-{TOP_N_PAIRS} FCA pairs and Bottom-{TOP_N_PAIRS} FCA pairs,\n",
    "   including bootstrap 95% CI for the difference in means.\n",
    "\n",
    "3) plots/\n",
    "   - scatter_fca_vs_bridge_share.png\n",
    "   - box_impact_top_vs_bottom.png\n",
    "   - violin_impact_top_vs_bottom.png\n",
    "\n",
    "Headline stats:\n",
    "---------------\n",
    "- Spearman(FCA_slope, share_bridge_authors) = {corr:.3f}\n",
    "- CPR mean difference (Top - Bottom) = {diff:.4f}\n",
    "  95% CI: [{ci_lo:.4f}, {ci_hi:.4f}]\n",
    "\n",
    "Interpretation guide:\n",
    "---------------------\n",
    "- A positive Spearman correlation indicates that field pairs with faster convergence\n",
    "  tend to involve a larger share of bridge authors (BES top-quantile).\n",
    "- A positive CPR difference (Top - Bottom) suggests that papers on fast-converging pairs\n",
    "  are associated with higher impact (citations per reference).\n",
    "\"\"\"\n",
    "    with open(os.path.join(out_dir, \"README.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    print(\"\\n✅ Done Part C!\")\n",
    "    print(f\"- Saved: {link_path}\")\n",
    "    print(f\"- Saved: {impact_path}\")\n",
    "    print(f\"- Plots: {plots_dir}\")\n",
    "    print(f\"- Output folder: {out_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af55f36-c46d-4220-ba6e-4d6ce3bd7d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_310)",
   "language": "python",
   "name": "env_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
