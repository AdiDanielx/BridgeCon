{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e65271-2e01-4d50-95b5-006fdc10d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Null] iteration 30/300 done in 9.9s\n",
      "[Null] iteration 60/300 done in 9.9s\n",
      "[Null] iteration 90/300 done in 9.3s\n",
      "[Null] iteration 120/300 done in 9.5s\n",
      "[Null] iteration 150/300 done in 10.2s\n",
      "[Null] iteration 180/300 done in 10.1s\n",
      "[Null] iteration 210/300 done in 10.0s\n",
      "[Null] iteration 240/300 done in 10.0s\n",
      "[Null] iteration 270/300 done in 9.8s\n",
      "[Null] iteration 300/300 done in 9.9s\n",
      "\n",
      "✅ Part E complete!\n",
      "- Significance table: e_significance_20250816_134030/fca_significance.csv\n",
      "- Significant pairs:  e_significance_20250816_134030/significant_pairs_topK.csv\n",
      "- Cases folder:       e_significance_20250816_134030/cases\n",
      "- Output folder:      e_significance_20250816_134030\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Part E — FCA Significance (Null Model) + Bridge Case Studies\n",
    "Assumes Parts A, B, C are done.\n",
    "\n",
    "Outputs under: e_significance_<timestamp>/\n",
    "  - fca_significance.csv           (observed vs null; p-values + FDR)\n",
    "  - significant_pairs_topK.csv     (top significant FCA pairs)\n",
    "  - cases/ per-pair CSVs with top bridge authors + counts + CPR\n",
    "  - plots/ (optional small figures if enabled)\n",
    "  - README.txt\n",
    "\"\"\"\n",
    "\n",
    "import os, json, math, itertools, time\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "YEARS = list(range(2018, 2026))           # 2018–2025\n",
    "ARTICLES_DIR_TEMPLATE = \"articles_{year}_new\"\n",
    "ARTICLES_FILE = \"all_articles_enhanced.jsonl\"\n",
    "\n",
    "N_NULL = 300                               # איטרציות למודל האפס (אפשר 200–500)\n",
    "TOP_CASES = 20                             # כמה מחברי-גשר להציג לכל זוג\n",
    "BRIDGE_QUANTILE = 0.90                     # הגדרת מחברי-גשר (כמו ב-C/D)\n",
    "MIN_YEARS_PER_PAIR = 3                     # כמו ב-A\n",
    "LIMIT_PAIRS_TO_FCA = True                  # True: מחשבים מובהקות רק לזוגות שקיבלו FCA ב-A (יעיל יותר)\n",
    "RANDOM_SEED = 7\n",
    "# ----------------------------------------\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "def find_latest_dir(prefix: str):\n",
    "    cands = [d for d in os.listdir(\".\") if d.startswith(prefix) and os.path.isdir(d)]\n",
    "    if not cands: return None\n",
    "    cands.sort(reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "def ensure_outdir():\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = f\"e_significance_{ts}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(out_dir, \"cases\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(out_dir, \"plots\"), exist_ok=True)\n",
    "    return out_dir\n",
    "\n",
    "def safe_pair(a,b):\n",
    "    return tuple(sorted((str(a), str(b))))\n",
    "\n",
    "# ---------- Load Part A/B/C ----------\n",
    "def load_partA():\n",
    "    a_dir = find_latest_dir(\"field_convergence_A_\")\n",
    "    if not a_dir: raise FileNotFoundError(\"Missing field_convergence_A_*\")\n",
    "    fca_path = os.path.join(a_dir, \"fca_summary.csv\")\n",
    "    per_year_w_path = os.path.join(a_dir, \"per_year_field_weights.csv\")\n",
    "    if not os.path.exists(fca_path) or not os.path.exists(per_year_w_path):\n",
    "        raise FileNotFoundError(\"Part A outputs missing\")\n",
    "    df_fca = pd.read_csv(fca_path)\n",
    "    df_fca[\"pair\"] = list(map(lambda xy: safe_pair(xy[0], xy[1]), zip(df_fca[\"field_i\"], df_fca[\"field_j\"])))\n",
    "    df_fca = df_fca[df_fca[\"years_covered\"] >= MIN_YEARS_PER_PAIR].copy()\n",
    "    return a_dir, df_fca, per_year_w_path\n",
    "\n",
    "def load_partB():\n",
    "    b_dir = find_latest_dir(\"bridge_emergence_B_\")\n",
    "    if not b_dir: raise FileNotFoundError(\"Missing bridge_emergence_B_*\")\n",
    "    bes_path = os.path.join(b_dir, \"bes_summary.csv\")\n",
    "    if not os.path.exists(bes_path): raise FileNotFoundError(\"Missing bes_summary.csv\")\n",
    "    df_bes = pd.read_csv(bes_path)\n",
    "    df_bes[\"author_id\"] = df_bes[\"author_id\"].astype(str)\n",
    "    thr = df_bes[\"BES_delta\"].quantile(BRIDGE_QUANTILE)\n",
    "    bridge_authors = set(df_bes.loc[df_bes[\"BES_delta\"] >= thr, \"author_id\"])\n",
    "    return b_dir, df_bes, bridge_authors\n",
    "\n",
    "# ---------- Read articles ----------\n",
    "def stream_articles(year):\n",
    "    folder = ARTICLES_DIR_TEMPLATE.format(year=year)\n",
    "    path = os.path.join(folder, ARTICLES_FILE)\n",
    "    if not os.path.exists(path): return\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "def collect_paper_fields_for_year(year):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      papers: list of (paper_id, fields_unique_sorted)\n",
    "      field_counts: Counter of field frequency across papers (1 per paper)\n",
    "      authors_per_paper: dict paper_id -> [author_ids]\n",
    "      cpr_per_paper: dict paper_id -> CPR (citations per reference)\n",
    "    \"\"\"\n",
    "    papers = []\n",
    "    field_counts = Counter()\n",
    "    authors_per_paper = {}\n",
    "    cpr_per_paper = {}\n",
    "    for obj in stream_articles(year) or []:\n",
    "        fields = obj.get(\"fields\") or []\n",
    "        fields = [str(x).strip() for x in fields if x and str(x).strip()]\n",
    "        fields = sorted(set(fields))\n",
    "        if not fields:\n",
    "            continue\n",
    "        pid = obj.get(\"id\")\n",
    "        papers.append((pid, fields))\n",
    "        field_counts.update(fields)\n",
    "\n",
    "        # authors\n",
    "        authors = obj.get(\"authors\") or []\n",
    "        aids = []\n",
    "        for a in authors:\n",
    "            aid = a.get(\"author_id\")\n",
    "            if aid: aids.append(str(aid))\n",
    "        authors_per_paper[pid] = list(dict.fromkeys(aids))\n",
    "\n",
    "        # CPR\n",
    "        cpr = obj.get(\"citation_per_reference_ratio\")\n",
    "        if cpr is None:\n",
    "            ref = obj.get(\"referenced_works_count\") or 1\n",
    "            cited = obj.get(\"cited_by_count\") or 0\n",
    "            cpr = float(cited)/float(ref)\n",
    "        cpr_per_paper[pid] = float(cpr)\n",
    "\n",
    "    return papers, field_counts, authors_per_paper, cpr_per_paper\n",
    "\n",
    "def pair_counts_from_papers(papers):\n",
    "    \"\"\"\n",
    "    Given list[(pid, fields)], compute:\n",
    "      field_count: Counter field->#papers\n",
    "      pair_count : Counter (f_i,f_j)->#papers\n",
    "    \"\"\"\n",
    "    field_count = Counter()\n",
    "    pair_count = Counter()\n",
    "    for pid, fields in papers:\n",
    "        field_count.update(fields)\n",
    "        if len(fields) > 1:\n",
    "            for a,b in itertools.combinations(fields, 2):\n",
    "                pair_count[safe_pair(a,b)] += 1\n",
    "    return field_count, pair_count\n",
    "\n",
    "def cosine_weight(co_ij, c_i, c_j, eps=1e-12):\n",
    "    denom = math.sqrt(max(c_i,0)*max(c_j,0)) + eps\n",
    "    return co_ij/denom\n",
    "\n",
    "# ---------- Null model ----------\n",
    "def sample_fields_for_paper(L, field_universe, probs):\n",
    "    \"\"\"\n",
    "    Weighted sample without replacement of L fields.\n",
    "    If L > len(universe), fall back to with-replacement then unique.\n",
    "    \"\"\"\n",
    "    L = int(L)\n",
    "    if L <= 0:\n",
    "        return []\n",
    "    if L >= len(field_universe):\n",
    "        # edge case\n",
    "        return field_universe[:]\n",
    "    idxs = rng.choice(len(field_universe), size=L, replace=False, p=probs)\n",
    "    return [field_universe[i] for i in idxs]\n",
    "\n",
    "def build_null_year_papers(real_papers, field_counts):\n",
    "    \"\"\"\n",
    "    For a given year:\n",
    "      - keep #fields per paper\n",
    "      - sample fields for each paper from weighted global distribution\n",
    "    \"\"\"\n",
    "    fields = list(field_counts.keys())\n",
    "    weights = np.array([field_counts[f] for f in fields], dtype=float)\n",
    "    probs = weights/weights.sum() if weights.sum()>0 else np.ones_like(weights)/len(weights)\n",
    "\n",
    "    null_papers = []\n",
    "    for pid, real_fields in real_papers:\n",
    "        L = len(real_fields)\n",
    "        nf = sample_fields_for_paper(L, fields, probs)\n",
    "        null_papers.append((pid, sorted(set(nf))))\n",
    "    return null_papers\n",
    "\n",
    "# ---------- FCA slope ----------\n",
    "def slope(xs, ys):\n",
    "    xs = np.asarray(xs, float); ys = np.asarray(ys, float)\n",
    "    if len(xs) < 2: return np.nan\n",
    "    m, b = np.polyfit(xs, ys, 1)\n",
    "    return float(m)\n",
    "\n",
    "def compute_year_weights_for_pairs(papers_by_year, target_pairs):\n",
    "    \"\"\"\n",
    "    For each year, compute cosine weight for target_pairs only (efficient).\n",
    "    Returns dict: pair -> list[(year, weight)]\n",
    "    \"\"\"\n",
    "    out = defaultdict(list)\n",
    "    for year, papers in papers_by_year.items():\n",
    "        field_count, pair_count = pair_counts_from_papers(papers)\n",
    "        for (a,b) in target_pairs:\n",
    "            co = pair_count.get((a,b), 0)\n",
    "            w = cosine_weight(co, field_count.get(a,0), field_count.get(b,0))\n",
    "            out[(a,b)].append((year, w))\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    out_dir = ensure_outdir()\n",
    "\n",
    "    # Load A/B\n",
    "    a_dir, df_fca, _ = load_partA()\n",
    "    b_dir, df_bes, bridge_authors = load_partB()\n",
    "\n",
    "    # -------- Target pairs for significance --------\n",
    "    if LIMIT_PAIRS_TO_FCA:\n",
    "        target_pairs = set(df_fca[\"pair\"].tolist())  # כל הזוגות שקיבלו FCA ב-A\n",
    "    else:\n",
    "        target_pairs = set(df_fca[\"pair\"].tolist())\n",
    "\n",
    "    # -------- Collect real data per year --------\n",
    "    real_papers_by_year = {}\n",
    "    authors_per_paper_by_year = {}\n",
    "    cpr_per_paper_by_year = {}\n",
    "    field_counts_by_year = {}\n",
    "\n",
    "    for y in YEARS:\n",
    "        papers, field_counts, a_per_p, cpr_per_p = collect_paper_fields_for_year(y)\n",
    "        if papers:\n",
    "            real_papers_by_year[y] = papers\n",
    "            field_counts_by_year[y] = field_counts\n",
    "            authors_per_paper_by_year[y] = a_per_p\n",
    "            cpr_per_paper_by_year[y] = cpr_per_p\n",
    "\n",
    "    # -------- Observed slopes for target pairs --------\n",
    "    obs_weights = compute_year_weights_for_pairs(real_papers_by_year, target_pairs)\n",
    "    obs_rows = []\n",
    "    for pair, lst in obs_weights.items():\n",
    "        lst = sorted(lst, key=lambda x: x[0])\n",
    "        years = [t for t,_ in lst]\n",
    "        vals  = [w for _,w in lst]\n",
    "        if len(years) < MIN_YEARS_PER_PAIR: \n",
    "            continue\n",
    "        m = slope(years, vals)\n",
    "        obs_rows.append({\"pair\":pair, \"field_i\":pair[0], \"field_j\":pair[1], \"obs_slope\":m, \n",
    "                         \"years_covered\":len(years), \"year_first\":min(years), \"year_last\":max(years)})\n",
    "    df_obs = pd.DataFrame(obs_rows)\n",
    "\n",
    "    # -------- Null slopes (permutation across years) --------\n",
    "    # Precompute per-year null papers N_NULL times and reuse for all pairs\n",
    "    null_slopes = defaultdict(list)  # pair -> [m1,m2,...]\n",
    "\n",
    "    for it in range(N_NULL):\n",
    "        t0 = time.time()\n",
    "        null_papers_by_year = {}\n",
    "        for y in YEARS:\n",
    "            if y not in real_papers_by_year: continue\n",
    "            rp = real_papers_by_year[y]\n",
    "            fc = field_counts_by_year[y]\n",
    "            null_papers_by_year[y] = build_null_year_papers(rp, fc)\n",
    "\n",
    "        null_weights = compute_year_weights_for_pairs(null_papers_by_year, target_pairs)\n",
    "        for pair, lst in null_weights.items():\n",
    "            lst = sorted(lst, key=lambda x: x[0])\n",
    "            years = [t for t,_ in lst]\n",
    "            vals  = [w for _,w in lst]\n",
    "            if len(years) < MIN_YEARS_PER_PAIR:\n",
    "                continue\n",
    "            m = slope(years, vals)\n",
    "            null_slopes[pair].append(m)\n",
    "\n",
    "        if (it+1) % max(1, N_NULL//10) == 0:\n",
    "            print(f\"[Null] iteration {it+1}/{N_NULL} done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    # -------- Significance table --------\n",
    "    rows = []\n",
    "    for _, r in df_obs.iterrows():\n",
    "        pair = r[\"pair\"]\n",
    "        null_list = null_slopes.get(pair, [])\n",
    "        if len(null_list) == 0:\n",
    "            rows.append({**r.to_dict(), \"null_mean\":np.nan, \"null_std\":np.nan,\n",
    "                         \"z\":np.nan, \"p_one_sided\":np.nan})\n",
    "            continue\n",
    "        arr = np.asarray(null_list, float)\n",
    "        mu  = float(np.mean(arr))\n",
    "        sd  = float(np.std(arr, ddof=1)) if len(arr)>1 else np.nan\n",
    "        # one-sided p for positive acceleration (obs larger than null)\n",
    "        p = float((np.sum(arr >= r[\"obs_slope\"]) + 1) / (len(arr) + 1))\n",
    "        z = (r[\"obs_slope\"]-mu)/sd if (sd and sd>0) else np.nan\n",
    "        rows.append({**r.to_dict(), \"null_mean\":mu, \"null_std\":sd, \"z\":z, \"p_one_sided\":p})\n",
    "\n",
    "    df_sig = pd.DataFrame(rows).sort_values([\"p_one_sided\",\"obs_slope\"], ascending=[True,False]).reset_index(drop=True)\n",
    "\n",
    "    # Benjamini-Hochberg FDR\n",
    "    def bh_fdr(pvals):\n",
    "        p = np.asarray(pvals, float)\n",
    "        n = len(p)\n",
    "        order = np.argsort(p)\n",
    "        ranked = p[order]\n",
    "        q = np.empty(n, float); q[:] = np.nan\n",
    "        min_coeff = 1.0\n",
    "        for i in range(n-1, -1, -1):\n",
    "            rank = i+1\n",
    "            val = ranked[i]*n/rank\n",
    "            if val < min_coeff: min_coeff = val\n",
    "            q[i] = min(1.0, min_coeff)\n",
    "        out = np.empty(n, float); out[:] = np.nan\n",
    "        out[order] = q\n",
    "        return out\n",
    "\n",
    "    if \"p_one_sided\" in df_sig.columns:\n",
    "        df_sig[\"q_fdr\"] = bh_fdr(df_sig[\"p_one_sided\"].fillna(1.0).values)\n",
    "    else:\n",
    "        df_sig[\"q_fdr\"] = np.nan\n",
    "\n",
    "    out_dir = ensure_outdir()\n",
    "    sig_path = os.path.join(out_dir, \"fca_significance.csv\")\n",
    "    df_sig.to_csv(sig_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Save top significant pairs (q<=0.05 by default)\n",
    "    top_sig = df_sig[df_sig[\"q_fdr\"] <= 0.05].copy()\n",
    "    if top_sig.empty:\n",
    "        top_sig = df_sig.head(50).copy()  # fallback\n",
    "    top_sig_path = os.path.join(out_dir, \"significant_pairs_topK.csv\")\n",
    "    top_sig.to_csv(top_sig_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # -------- Case studies: who are the bridge authors on these pairs? --------\n",
    "    pairs_set = set(map(tuple, top_sig[\"pair\"].values))\n",
    "    case_rows_all = []\n",
    "    for y in YEARS:\n",
    "        for obj in stream_articles(y) or []:\n",
    "            fields = obj.get(\"fields\") or []\n",
    "            fields = [str(x).strip() for x in fields if x and str(x).strip()]\n",
    "            fields = sorted(set(fields))\n",
    "            if len(fields) < 2: continue\n",
    "            paper_pairs = set(safe_pair(a,b) for a,b in itertools.combinations(fields,2))\n",
    "            hits = paper_pairs & pairs_set\n",
    "            if not hits: continue\n",
    "\n",
    "            # CPR\n",
    "            cpr = obj.get(\"citation_per_reference_ratio\")\n",
    "            if cpr is None:\n",
    "                ref = obj.get(\"referenced_works_count\") or 1\n",
    "                cited = obj.get(\"cited_by_count\") or 0\n",
    "                cpr = float(cited)/float(ref)\n",
    "            cpr = float(cpr)\n",
    "\n",
    "            # authors\n",
    "            aids = []\n",
    "            for a in obj.get(\"authors\") or []:\n",
    "                aid = a.get(\"author_id\")\n",
    "                if aid: aids.append(str(aid))\n",
    "            aids = list(dict.fromkeys(aids))\n",
    "\n",
    "            for p in hits:\n",
    "                for aid in aids:\n",
    "                    case_rows_all.append({\n",
    "                        \"year\": y, \"pair\": p, \"field_i\": p[0], \"field_j\": p[1],\n",
    "                        \"author_id\": aid,\n",
    "                        \"is_bridge\": (aid in bridge_authors),\n",
    "                        \"CPR\": cpr\n",
    "                    })\n",
    "\n",
    "    if case_rows_all:\n",
    "        df_cases_all = pd.DataFrame(case_rows_all)\n",
    "        # aggregate per pair & author\n",
    "        g = df_cases_all.groupby([\"pair\",\"author_id\"], as_index=False).agg(\n",
    "            n_papers=(\"year\",\"count\"),\n",
    "            mean_CPR=(\"CPR\",\"mean\"),\n",
    "            is_bridge=(\"is_bridge\",\"max\")\n",
    "        )\n",
    "        # write per-pair CSVs with top bridge authors first\n",
    "        for pair, sub in g.groupby(\"pair\"):\n",
    "            sub = sub.sort_values([\"is_bridge\",\"n_papers\",\"mean_CPR\"], ascending=[False,False,False])\n",
    "            sub.head(TOP_CASES).to_csv(\n",
    "                os.path.join(out_dir, \"cases\", f\"case_{pair[0].replace('/','-')}__{pair[1].replace('/','-')}.csv\"),\n",
    "                index=False, encoding=\"utf-8\"\n",
    "            )\n",
    "    else:\n",
    "        df_cases_all = pd.DataFrame()\n",
    "\n",
    "    # README\n",
    "    readme = f\"\"\"Part E — FCA Significance + Bridge Case Studies (2018–2025)\n",
    "==================================================================\n",
    "\n",
    "What this does:\n",
    "---------------\n",
    "1) FCA Significance:\n",
    "   - Builds a year-wise null model by reassigning fields to papers\n",
    "     (preserving the number of fields per paper and the global field frequencies per year).\n",
    "   - Recomputes pairwise cosine weights and FCA slopes across years for N={N_NULL} iterations.\n",
    "   - Reports one-sided p-values (obs slope > null) and BH FDR (q-values).\n",
    "\n",
    "2) Case Studies:\n",
    "   - For FCA-significant field pairs (q<=0.05), lists the most active authors in those pairs,\n",
    "     highlighting which are \"bridge authors\" (top {int((1-BRIDGE_QUANTILE)*100)}% BES).\n",
    "   - Includes per-author paper counts and mean CPR for those pair-specific papers.\n",
    "\n",
    "Key outputs:\n",
    "------------\n",
    "- fca_significance.csv        (obs_slope, null_mean/std, z, p_one_sided, q_fdr)\n",
    "- significant_pairs_topK.csv  (filtered set, default q<=0.05 or top-50 fallback)\n",
    "- cases/*.csv                 (per-pair author lists: n_papers, mean_CPR, is_bridge)\n",
    "\n",
    "Notes:\n",
    "------\n",
    "- Years used: 2018–2025.\n",
    "- Null model is approximate; tighten N_NULL for more stable p-values.\n",
    "- To speed up, LIMIT_PAIRS_TO_FCA=True limits to pairs that already had FCA in Part A.\n",
    "\"\"\"\n",
    "    with open(os.path.join(out_dir, \"README.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    print(\"\\n✅ Part E complete!\")\n",
    "    print(f\"- Significance table: {sig_path}\")\n",
    "    print(f\"- Significant pairs:  {top_sig_path}\")\n",
    "    print(f\"- Cases folder:       {os.path.join(out_dir, 'cases')}\")\n",
    "    print(f\"- Output folder:      {out_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29b191-9912-4e50-b342-5bfab8529028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_310)",
   "language": "python",
   "name": "env_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
