{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e51690-951b-4560-843f-ae44c873b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "YEARS = list(range(2018, 2026))\n",
    "INPUT_FOLDER_TEMPLATE = \"articles_{year}_new\"     \n",
    "INPUT_FILE_NAME = \"all_articles_enhanced.jsonl\"  \n",
    "OUTPUT_ROOT = \"bridge_emergence_B\"\n",
    "\n",
    "MIN_DEGREE_PER_YEAR = 1\n",
    "\n",
    "MIN_PAPERS_PER_AUTHOR_PER_YEAR = 1  \n",
    "\n",
    "def read_year_items(year):\n",
    "    \"\"\"\n",
    "    Load the JSONL file for a given year.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        Publication year.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        List of enriched article records for that year.\n",
    "    \"\"\"\n",
    "\n",
    "    folder = INPUT_FOLDER_TEMPLATE.format(year=year)\n",
    "    path = os.path.join(folder, INPUT_FILE_NAME)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"⚠ File not found for year {year}: {path}\")\n",
    "        return []\n",
    "\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                items.append(obj)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return items\n",
    "\n",
    "\n",
    "def extract_authors_from_item(item):\n",
    "    \"\"\"\n",
    "    Extract a list of author IDs from an article record.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    item : dict\n",
    "        Enriched article object containing an 'authors' field.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        Unique list of author IDs for this article.\n",
    "    \"\"\"\n",
    "    authors = []\n",
    "    for a in item.get(\"authors\", []) or []:\n",
    "        aid = a.get(\"author_id\")\n",
    "        if aid:\n",
    "            authors.append(aid)\n",
    "    authors = list(dict.fromkeys(authors))\n",
    "    return authors\n",
    "\n",
    "\n",
    "def build_coauthor_graph(year_items):\n",
    "    \"\"\"\n",
    "    Build a co-authorship graph for a given year's articles.\n",
    "\n",
    "    Nodes = authors\n",
    "    Edges = co-authorships in that year (weighted by number of shared papers)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year_items : list of dict\n",
    "        List of enriched article records.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    G : networkx.Graph\n",
    "        Undirected weighted graph of authors.\n",
    "    author_paper_count : collections.Counter\n",
    "        Number of papers per author in that year.\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    author_paper_count = Counter()\n",
    "\n",
    "    for it in year_items:\n",
    "        authors = extract_authors_from_item(it)\n",
    "        if not authors or len(authors) == 1:\n",
    "            # גם יחיד: נרשום קיום מחבר במערכת, כדי לא לאבדו\n",
    "            for a in authors:\n",
    "                author_paper_count[a] += 1\n",
    "                if a not in G:\n",
    "                    G.add_node(a)\n",
    "            continue\n",
    "\n",
    "        for a in authors:\n",
    "            author_paper_count[a] += 1\n",
    "            if a not in G:\n",
    "                G.add_node(a)\n",
    "\n",
    "        for i in range(len(authors)):\n",
    "            for j in range(i + 1, len(authors)):\n",
    "                u, v = authors[i], authors[j]\n",
    "                if G.has_edge(u, v):\n",
    "                    G[u][v]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(u, v, weight=1)\n",
    "\n",
    "    return G, author_paper_count\n",
    "\n",
    "\n",
    "def participation_coefficient(G, communities):\n",
    "    \"\"\"\n",
    "    Compute the participation coefficient for each author in a given graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        Co-authorship graph for a given year.\n",
    "    communities : list of set\n",
    "        List of sets, each set representing a detected community of nodes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    P : dict\n",
    "        Mapping from author_id -> participation coefficient [0,1].\n",
    "    comm_id_map : dict\n",
    "        Mapping from author_id -> community ID.\n",
    "    \"\"\"\n",
    "    comm_id_map = {}\n",
    "    for idx, comm in enumerate(communities):\n",
    "        for node in comm:\n",
    "            comm_id_map[node] = idx\n",
    "\n",
    "    P = {}\n",
    "    for a in G.nodes():\n",
    "        k_a = G.degree(a, weight=None)  \n",
    "        if k_a == 0:\n",
    "            P[a] = np.nan \n",
    "            continue\n",
    "\n",
    "        neighbor_comm_counts = Counter()\n",
    "        for nb in G.neighbors(a):\n",
    "            c = comm_id_map.get(nb, -1)\n",
    "            neighbor_comm_counts[c] += 1\n",
    "\n",
    "        s = 0.0\n",
    "        for c, k_ac in neighbor_comm_counts.items():\n",
    "            frac = k_ac / k_a\n",
    "            s += frac * frac\n",
    "        P[a] = 1.0 - s\n",
    "\n",
    "    return P, comm_id_map\n",
    "\n",
    "\n",
    "def ensure_output_dir():\n",
    "    \"\"\"\n",
    "    Create a timestamped output directory under OUTPUT_ROOT.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Path to created output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = f\"{OUTPUT_ROOT}_{ts}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main pipeline for computing Bridge Emergence Score (BES).\n",
    "\n",
    "    Steps:\n",
    "    ------\n",
    "    1. For each year:\n",
    "       - Build co-author graph.\n",
    "       - Filter authors by minimum paper and degree thresholds.\n",
    "       - Detect communities (Greedy Modularity).\n",
    "       - Compute participation coefficient for each author.\n",
    "       - Store per-year results.\n",
    "\n",
    "    2. For each author across years:\n",
    "       - Identify valid active years (degree >= MIN_DEGREE_PER_YEAR).\n",
    "       - Compute BES = P_last - P_first.\n",
    "\n",
    "    3. Save results:\n",
    "       - per_year_author_participation.csv\n",
    "       - bes_summary.csv\n",
    "       - top_emerging_bridges.csv\n",
    "       - README.txt (English description).\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = ensure_output_dir()\n",
    "\n",
    "    yearly_rows = [] \n",
    "    authors_year_P = defaultdict(dict)  \n",
    "    authors_year_deg = defaultdict(dict)  \n",
    "    authors_year_papers = defaultdict(dict)  \n",
    "\n",
    "    for year in YEARS:\n",
    "        items = read_year_items(year)\n",
    "        if not items:\n",
    "            continue\n",
    "\n",
    "        G, papers_cnt = build_coauthor_graph(items)\n",
    "\n",
    "        keep_nodes = {a for a, c in papers_cnt.items() if c >= MIN_PAPERS_PER_AUTHOR_PER_YEAR}\n",
    "        if keep_nodes:\n",
    "            G = G.subgraph(keep_nodes).copy()\n",
    "            papers_cnt = Counter({a: c for a, c in papers_cnt.items() if a in keep_nodes})\n",
    "\n",
    "        if G.number_of_nodes() == 0:\n",
    "            continue\n",
    "\n",
    "        communities = list(greedy_modularity_communities(G))\n",
    "\n",
    "        P_map, comm_id_map = participation_coefficient(G, communities)\n",
    "\n",
    "        deg_map = dict(G.degree(weight=None))\n",
    "\n",
    "        for a in G.nodes():\n",
    "            P = P_map.get(a, np.nan)\n",
    "            deg = deg_map.get(a, 0)\n",
    "            papers = papers_cnt.get(a, 0)\n",
    "\n",
    "            if deg < MIN_DEGREE_PER_YEAR:\n",
    "                pass\n",
    "\n",
    "            yearly_rows.append({\n",
    "                \"year\": year,\n",
    "                \"author_id\": a,\n",
    "                \"degree\": deg,\n",
    "                \"papers\": papers,\n",
    "                \"participation\": P,\n",
    "                \"community_id\": comm_id_map.get(a, -1),\n",
    "                \"num_communities_year\": len(communities),\n",
    "                \"num_nodes_year\": G.number_of_nodes(),\n",
    "                \"num_edges_year\": G.number_of_edges()\n",
    "            })\n",
    "            authors_year_P[a][year] = P\n",
    "            authors_year_deg[a][year] = deg\n",
    "            authors_year_papers[a][year] = papers\n",
    "\n",
    "    if not yearly_rows:\n",
    "        print(\"No yearly rows produced. Check input folders/files.\")\n",
    "        return\n",
    "\n",
    "    df_yearly = pd.DataFrame(yearly_rows)\n",
    "    df_yearly = df_yearly.sort_values([\"author_id\", \"year\"]).reset_index(drop=True)\n",
    "\n",
    "    bes_rows = []\n",
    "    for a, year_to_P in authors_year_P.items():\n",
    "        valid_years = []\n",
    "        for y, P in year_to_P.items():\n",
    "            deg = authors_year_deg[a].get(y, 0)\n",
    "            if deg >= MIN_DEGREE_PER_YEAR and P is not None and not (isinstance(P, float) and np.isnan(P)):\n",
    "                valid_years.append(y)\n",
    "        if len(valid_years) < 2:\n",
    "            continue\n",
    "\n",
    "        ys = sorted(valid_years)\n",
    "        first_y, last_y = ys[0], ys[-1]\n",
    "        P_first = year_to_P[first_y]\n",
    "        P_last  = year_to_P[last_y]\n",
    "\n",
    "        bes = float(P_last - P_first)\n",
    "\n",
    "        bes_rows.append({\n",
    "            \"author_id\": a,\n",
    "            \"year_first\": int(first_y),\n",
    "            \"year_last\": int(last_y),\n",
    "            \"P_first\": float(P_first),\n",
    "            \"P_last\": float(P_last),\n",
    "            \"BES_delta\": bes,\n",
    "            \"years_active\": len(ys),\n",
    "            \"mean_degree_active_years\": float(np.mean([authors_year_deg[a][y] for y in ys])),\n",
    "            \"mean_papers_active_years\": float(np.mean([authors_year_papers[a][y] for y in ys]))\n",
    "        })\n",
    "\n",
    "    if not bes_rows:\n",
    "        print(\"No authors with ≥2 valid years. Consider lowering thresholds.\")\n",
    "        return\n",
    "\n",
    "    df_bes = pd.DataFrame(bes_rows)\n",
    "    df_bes = df_bes.sort_values(\"BES_delta\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # שמירות\n",
    "    per_year_path = os.path.join(out_dir, \"per_year_author_participation.csv\")\n",
    "    bes_path = os.path.join(out_dir, \"bes_summary.csv\")\n",
    "    top_path = os.path.join(out_dir, \"top_emerging_bridges.csv\")\n",
    "\n",
    "    df_yearly.to_csv(per_year_path, index=False, encoding=\"utf-8\")\n",
    "    df_bes.to_csv(bes_path, index=False, encoding=\"utf-8\")\n",
    "    # Top emerging bridges (לפי BES_delta)\n",
    "    df_bes.head(1000).to_csv(top_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_310)",
   "language": "python",
   "name": "env_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
