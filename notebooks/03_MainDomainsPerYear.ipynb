{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa366e-a990-4424-a0cc-f8906ff91431",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "KNOWN_DOMAINS = {\n",
    "    # STEM Core\n",
    "    \"Medicine\", \"Biology\", \"Chemistry\", \"Physics\", \"Mathematics\", \"Computer Science\",\n",
    "    \"Engineering\", \"Earth Science\", \"Environmental Science\", \"Materials Science\",\n",
    "    \"Astronomy\", \"Astrophysics\", \"Geology\", \"Meteorology\", \"Oceanography\",\n",
    "    \n",
    "    # Life Sciences\n",
    "    \"Genetics\", \"Molecular Biology\", \"Cell Biology\", \"Biochemistry\", \"Microbiology\",\n",
    "    \"Immunology\", \"Pharmacology\", \"Pathology\", \"Physiology\", \"Anatomy\",\n",
    "    \"Botany\", \"Zoology\", \"Ecology\", \"Evolutionary Biology\", \"Marine Biology\",\n",
    "    \n",
    "    # Medical & Health\n",
    "    \"Public Health\", \"Epidemiology\", \"Clinical Medicine\", \"Surgery\", \"Psychiatry\",\n",
    "    \"Neurology\", \"Cardiology\", \"Oncology\", \"Pediatrics\", \"Geriatrics\",\n",
    "    \"Radiology\", \"Anesthesiology\", \"Emergency Medicine\", \"Family Medicine\",\n",
    "    \n",
    "    # Technology & Computing\n",
    "    \"Artificial Intelligence\", \"Machine Learning\", \"Data Science\", \"Robotics\",\n",
    "    \"Software Engineering\", \"Cybersecurity\", \"Human-Computer Interaction\",\n",
    "    \"Information Systems\", \"Telecommunications\", \"Biotechnology\",\n",
    "    \"Nanotechnology\", \"Quantum Computing\", \"Bioinformatics\",\n",
    "    \n",
    "    # Engineering Disciplines\n",
    "    \"Mechanical Engineering\", \"Electrical Engineering\", \"Civil Engineering\",\n",
    "    \"Chemical Engineering\", \"Aerospace Engineering\", \"Biomedical Engineering\",\n",
    "    \"Environmental Engineering\", \"Industrial Engineering\", \"Nuclear Engineering\",\n",
    "    \n",
    "    # Social Sciences\n",
    "    \"Psychology\", \"Sociology\", \"Political Science\", \"Economics\", \"Anthropology\",\n",
    "    \"Education\", \"Criminology\", \"Social Work\", \"International Relations\",\n",
    "    \"Public Policy\", \"Urban Planning\", \"Communication\",\n",
    "    \n",
    "    # Humanities & Arts\n",
    "    \"History\", \"Philosophy\", \"Literature\", \"Art\", \"Music\", \"Theater\",\n",
    "    \"Linguistics\", \"Languages\", \"Cultural Studies\", \"Religious Studies\",\n",
    "    \"Media Studies\", \"Film Studies\",\n",
    "    \n",
    "    # Business & Management\n",
    "    \"Business\", \"Management\", \"Marketing\", \"Finance\", \"Accounting\",\n",
    "    \"Operations Research\", \"Supply Chain\", \"Entrepreneurship\",\n",
    "    \n",
    "    # Interdisciplinary\n",
    "    \"Neuroscience\", \"Cognitive Science\", \"Environmental Studies\", \n",
    "    \"Climate Science\", \"Sustainability\", \"Gender Studies\", \"Area Studies\",\n",
    "    \"Science and Technology Studies\", \"Bioethics\", \"Digital Humanities\",\n",
    "    \n",
    "    # Geography & Earth\n",
    "    \"Geography\", \"Cartography\", \"Geographic Information Systems\", \"Remote Sensing\",\n",
    "    \n",
    "    # Law & Policy\n",
    "    \"Law\", \"Legal Studies\", \"Constitutional Law\", \"International Law\",\n",
    "    \n",
    "    # Applied Sciences\n",
    "    \"Agriculture\", \"Forestry\", \"Veterinary Science\", \"Food Science\",\n",
    "    \"Sports Science\", \"Nutrition\", \"Architecture\", \"Design\"\n",
    "}\n",
    "\n",
    "# Convert to set for O(1) lookup\n",
    "DOMAIN_SET = set(KNOWN_DOMAINS)\n",
    "\n",
    "def create_domain_keywords():\n",
    "    \"\"\"Create keyword mappings to domains for faster matching\"\"\"\n",
    "    keyword_to_domain = {}\n",
    "    \n",
    "    for domain in DOMAIN_SET:\n",
    "        keyword_to_domain[domain.lower()] = domain\n",
    "        \n",
    "        words = domain.lower().split()\n",
    "        for word in words:\n",
    "            if len(word) > 3:  # Only meaningful words\n",
    "                keyword_to_domain[word] = domain\n",
    "    \n",
    "    # Add specific keyword mappings\n",
    "    additional_mappings = {\n",
    "        # Technology\n",
    "        \"ai\": \"Artificial Intelligence\", \"ml\": \"Machine Learning\",\n",
    "        \"deep learning\": \"Machine Learning\", \"neural network\": \"Machine Learning\",\n",
    "        \"algorithm\": \"Computer Science\", \"programming\": \"Computer Science\",\n",
    "        \"software\": \"Software Engineering\", \"hardware\": \"Engineering\",\n",
    "        \"database\": \"Computer Science\", \"network\": \"Telecommunications\",\n",
    "        \n",
    "        # Medicine\n",
    "        \"medical\": \"Medicine\", \"clinical\": \"Clinical Medicine\", \"patient\": \"Medicine\",\n",
    "        \"treatment\": \"Medicine\", \"diagnosis\": \"Medicine\", \"therapy\": \"Medicine\",\n",
    "        \"disease\": \"Medicine\", \"health\": \"Public Health\", \"healthcare\": \"Medicine\",\n",
    "        \"hospital\": \"Medicine\", \"nursing\": \"Medicine\", \"pharmaceutical\": \"Pharmacology\",\n",
    "        \n",
    "        # Biology\n",
    "        \"cell\": \"Cell Biology\", \"gene\": \"Genetics\", \"dna\": \"Genetics\", \"rna\": \"Genetics\",\n",
    "        \"protein\": \"Biochemistry\", \"enzyme\": \"Biochemistry\", \"organism\": \"Biology\",\n",
    "        \"species\": \"Biology\", \"evolution\": \"Evolutionary Biology\", \"genome\": \"Genetics\",\n",
    "        \n",
    "        # Physics/Chemistry\n",
    "        \"quantum\": \"Physics\", \"particle\": \"Physics\", \"energy\": \"Physics\",\n",
    "        \"molecule\": \"Chemistry\", \"reaction\": \"Chemistry\", \"catalyst\": \"Chemistry\",\n",
    "        \"material\": \"Materials Science\", \"crystal\": \"Materials Science\",\n",
    "        \n",
    "        # Social Sciences\n",
    "        \"social\": \"Sociology\", \"society\": \"Sociology\", \"culture\": \"Anthropology\",\n",
    "        \"behavior\": \"Psychology\", \"psychological\": \"Psychology\", \"cognitive\": \"Psychology\",\n",
    "        \"economic\": \"Economics\", \"political\": \"Political Science\", \"policy\": \"Public Policy\",\n",
    "        \"education\": \"Education\", \"learning\": \"Education\", \"teaching\": \"Education\",\n",
    "        \n",
    "        # Environment\n",
    "        \"climate\": \"Climate Science\", \"environment\": \"Environmental Science\",\n",
    "        \"sustainability\": \"Sustainability\", \"ecology\": \"Ecology\", \"conservation\": \"Environmental Science\",\n",
    "        \"pollution\": \"Environmental Science\", \"renewable\": \"Environmental Science\",\n",
    "        \n",
    "        # Business\n",
    "        \"business\": \"Business\", \"management\": \"Management\", \"marketing\": \"Marketing\",\n",
    "        \"finance\": \"Finance\", \"economic\": \"Economics\", \"market\": \"Economics\",\n",
    "        \n",
    "        # Geography\n",
    "        \"geographic\": \"Geography\", \"spatial\": \"Geography\", \"mapping\": \"Geography\",\n",
    "        \"urban\": \"Urban Planning\", \"city\": \"Urban Planning\"\n",
    "    }\n",
    "    \n",
    "    keyword_to_domain.update(additional_mappings)\n",
    "    return keyword_to_domain\n",
    "\n",
    "def extract_domains_from_article_fast(article, keyword_to_domain):\n",
    "    \"\"\"\n",
    "    Fast domain extraction using keyword matching instead of fuzzy string matching\n",
    "    \"\"\"\n",
    "    found_domains = set()\n",
    "    \n",
    "    # Collect all text to search\n",
    "    text_sources = []\n",
    "    text_sources.extend(article.get(\"domains\", []))\n",
    "    text_sources.extend(article.get(\"fields\", []))\n",
    "    text_sources.extend(article.get(\"keywords\", []))\n",
    "    \n",
    "    # Also check title for domain keywords\n",
    "    title = article.get(\"title\", \"\")\n",
    "    if title:\n",
    "        text_sources.append(title)\n",
    "    \n",
    "    # Search for domain matches\n",
    "    for text in text_sources:\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Direct domain match\n",
    "        if text_lower in keyword_to_domain:\n",
    "            found_domains.add(keyword_to_domain[text_lower])\n",
    "            continue\n",
    "        \n",
    "        # Check if any keyword is contained in the text\n",
    "        for keyword, domain in keyword_to_domain.items():\n",
    "            if keyword in text_lower:\n",
    "                found_domains.add(domain)\n",
    "    \n",
    "    return list(found_domains)\n",
    "\n",
    "def build_domain_graph_fast(year, data_folder=\"articles_{year}_new\"):\n",
    "    \"\"\"Build domain graph with improved performance and statistics\"\"\"\n",
    "    file_path = os.path.join(data_folder.format(year=year), \"all_articles_enhanced.jsonl\")\n",
    "    if not os.path.exists(file_path):\n",
    "        return None, {}\n",
    "\n",
    "    keyword_to_domain = create_domain_keywords()\n",
    "    G = nx.Graph()\n",
    "    domain_pairs_counter = defaultdict(int)\n",
    "    domain_article_count = defaultdict(int)\n",
    "    total_articles = 0\n",
    "    multidisciplinary_articles = 0\n",
    "\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if line_num % 1000 == 0:\n",
    "                \n",
    "            try:\n",
    "                article = json.loads(line)\n",
    "                total_articles += 1\n",
    "                \n",
    "                domains = extract_domains_from_article_fast(article, keyword_to_domain)\n",
    "                \n",
    "                for domain in domains:\n",
    "                    domain_article_count[domain] += 1\n",
    "                \n",
    "                if len(domains) > 1:\n",
    "                    multidisciplinary_articles += 1\n",
    "                    # Add all possible pairs\n",
    "                    for d1, d2 in itertools.combinations(sorted(domains), 2):\n",
    "                        domain_pairs_counter[(d1, d2)] += 1\n",
    "                        \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    for (d1, d2), weight in domain_pairs_counter.items():\n",
    "        G.add_edge(d1, d2, weight=weight)\n",
    "\n",
    "    for domain in domain_article_count:\n",
    "        if domain not in G.nodes():\n",
    "            G.add_node(domain)\n",
    "\n",
    "    stats = {\n",
    "        \"total_articles\": total_articles,\n",
    "        \"multidisciplinary_articles\": multidisciplinary_articles,\n",
    "        \"total_domains\": len(domain_article_count),\n",
    "        \"connected_domains\": len([n for n in G.nodes() if G.degree(n) > 0]),\n",
    "        \"total_connections\": len(G.edges()),\n",
    "        \"top_domains\": sorted(domain_article_count.items(), key=lambda x: x[1], reverse=True)[:10],\n",
    "        \"strongest_connections\": sorted(domain_pairs_counter.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    }\n",
    "\n",
    "\n",
    "    return G, stats\n",
    "\n",
    "def save_graph_enhanced(G, year, stats, output_dir=\"domain_graphs\"):\n",
    "    \"\"\"Save graph with enhanced visualizations and statistics\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if len(G.nodes()) == 0:\n",
    "        print(f\"âš  No nodes to visualize for {year}\")\n",
    "        return\n",
    "\n",
    "    # Filter graph to show only meaningful connections\n",
    "    min_weight = max(1, len(G.edges()) // 100) if len(G.edges()) > 50 else 1\n",
    "    G_filtered = nx.Graph()\n",
    "    \n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if data['weight'] >= min_weight:\n",
    "            G_filtered.add_edge(u, v, weight=data['weight'])\n",
    "    \n",
    "    # If filtered graph is too small, use original\n",
    "    if len(G_filtered.nodes()) < 10:\n",
    "        G_filtered = G\n",
    "\n",
    "    pos = nx.spring_layout(G_filtered, seed=42, k=1, iterations=50)\n",
    "\n",
    "    # === MATPLOTLIB VISUALIZATION (JPG) ===\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Calculate node sizes based on degree\n",
    "    node_sizes = [G_filtered.degree(n) * 100 + 300 for n in G_filtered.nodes()]\n",
    "    \n",
    "    # Calculate edge widths\n",
    "    edge_weights = [G_filtered[u][v]['weight'] for u, v in G_filtered.edges()]\n",
    "    max_weight = max(edge_weights) if edge_weights else 1\n",
    "    edge_widths = [w/max_weight * 5 + 0.5 for w in edge_weights]\n",
    "\n",
    "    # Draw the graph\n",
    "    nx.draw_networkx_edges(G_filtered, pos, width=edge_widths, edge_color=\"lightgray\", alpha=0.6)\n",
    "    nx.draw_networkx_nodes(G_filtered, pos, node_size=node_sizes, \n",
    "                          node_color=range(len(G_filtered.nodes())), \n",
    "                          cmap=plt.cm.Set3, alpha=0.8)\n",
    "    nx.draw_networkx_labels(G_filtered, pos, font_size=8, font_weight='bold')\n",
    "    \n",
    "    plt.title(f\"Domain Network {year}\\n{stats['total_articles']} articles, {stats['total_domains']} domains, {stats['total_connections']} connections\", \n",
    "              fontsize=16, pad=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"domains_{year}.jpg\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # === PLOTLY VISUALIZATION (HTML) ===\n",
    "    # Prepare edge traces\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_info = []\n",
    "    \n",
    "    for u, v in G_filtered.edges():\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        edge_x += [x0, x1, None]\n",
    "        edge_y += [y0, y1, None]\n",
    "        weight = G_filtered[u][v]['weight']\n",
    "        edge_info.append(f\"{u} â†” {v}<br>Collaborations: {weight}\")\n",
    "\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.8, color='rgba(125, 125, 125, 0.5)'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "\n",
    "    # Prepare node traces\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_text = []\n",
    "    node_info = []\n",
    "    node_sizes = []\n",
    "    node_colors = []\n",
    "    \n",
    "    for i, node in enumerate(G_filtered.nodes()):\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        \n",
    "        # Calculate metrics for this node\n",
    "        degree = G_filtered.degree(node)\n",
    "        neighbors = list(G_filtered.neighbors(node))\n",
    "        \n",
    "        # Node size based on degree\n",
    "        size = min(degree * 8 + 15, 60)  # Cap maximum size\n",
    "        node_sizes.append(size)\n",
    "        node_colors.append(degree)\n",
    "        \n",
    "        # Node label\n",
    "        node_text.append(node)\n",
    "        \n",
    "        # Hover info\n",
    "        neighbor_text = \"<br>\".join([f\"â€¢ {n}\" for n in neighbors[:10]])  # Show max 10 neighbors\n",
    "        if len(neighbors) > 10:\n",
    "            neighbor_text += f\"<br>... and {len(neighbors)-10} more\"\n",
    "            \n",
    "        hover_text = f\"<b>{node}</b><br>\" \\\n",
    "                    f\"Connections: {degree}<br>\" \\\n",
    "                    f\"Connected to:<br>{neighbor_text}\"\n",
    "        node_info.append(hover_text)\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers+text',\n",
    "        text=node_text,\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(size=10, color=\"white\"),\n",
    "        hovertext=node_info,\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            size=node_sizes,\n",
    "            color=node_colors,\n",
    "            # colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title=\"Number of<br>Connections\",\n",
    "                tickmode=\"linear\",\n",
    "                thickness=15\n",
    "            ),\n",
    "            line=dict(width=2, color='white')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the figure\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title=dict(\n",
    "                            text=f\"<b>Domain Collaboration Network {year}</b><br>\" +\n",
    "                                 f\"<i>{stats['total_articles']:,} articles â€¢ {stats['total_domains']} domains â€¢ \" +\n",
    "                                 f\"{stats['total_connections']} connections â€¢ {stats['multidisciplinary_articles']:,} multidisciplinary</i>\",\n",
    "                            x=0.5,\n",
    "                            font=dict(size=18)\n",
    "                        ),\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=20, l=5, r=5, t=100),\n",
    "                        annotations=[\n",
    "                            dict(\n",
    "                                text=\"Node size = number of connections<br>\" +\n",
    "                                     \"Edge thickness = collaboration strength<br>\" +\n",
    "                                     \"Hover over nodes for details\",\n",
    "                                showarrow=False,\n",
    "                                xref=\"paper\", yref=\"paper\",\n",
    "                                x=0.005, y=-0.002,\n",
    "                                xanchor='left', yanchor='bottom',\n",
    "                                font=dict(size=12, color=\"gray\")\n",
    "                            )\n",
    "                        ],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        plot_bgcolor='white',\n",
    "                        paper_bgcolor='white'\n",
    "                    ))\n",
    "\n",
    "    # Save HTML\n",
    "    html_file = os.path.join(output_dir, f\"domains_{year}.html\")\n",
    "    fig.write_html(html_file)\n",
    "\n",
    "    # Save statistics\n",
    "    with open(os.path.join(output_dir, f\"stats_{year}.json\"), \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "\n",
    "\n",
    "# ========= Main execution =========\n",
    "def main():\n",
    "    years = [2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"\\nðŸ”„ Processing {year}...\")\n",
    "        G, stats = build_domain_graph_fast(year)\n",
    "        \n",
    "        if G and len(G.nodes()) > 0:\n",
    "            save_graph_enhanced(G, year, stats)\n",
    "        else:\n",
    "            print(f\"âš  No valid graph generated for {year}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695272f-d759-4ce6-9d06-e096a0b67b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d5e5380-0994-42d7-aed3-d97b27520185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Processing 2018...\n",
      "Processing articles for 2018...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "âœ… Graph built for 2018:\n",
      "   ðŸ“Š 15797 articles processed\n",
      "   ðŸ”¬ 122 unique domains found\n",
      "   ðŸ”— 5721 domain connections\n",
      "   ðŸŒ 15797 multidisciplinary articles\n",
      "   ðŸŽ¯ Using threshold: 298.0 (showing 574/5721 connections)\n",
      "ðŸ’¾ Saved graph and statistics for 2018:\n",
      "   ðŸ“Š JPG: domains_2018.jpg\n",
      "   ðŸŒ HTML: domains_2018.html\n",
      "   ðŸ“ˆ Stats: stats_2018.json\n",
      "\n",
      "ðŸ”„ Processing 2019...\n",
      "Processing articles for 2019...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "âœ… Graph built for 2019:\n",
      "   ðŸ“Š 15612 articles processed\n",
      "   ðŸ”¬ 122 unique domains found\n",
      "   ðŸ”— 5744 domain connections\n",
      "   ðŸŒ 15612 multidisciplinary articles\n",
      "   ðŸŽ¯ Using threshold: 298.6999999999998 (showing 575/5744 connections)\n",
      "ðŸ’¾ Saved graph and statistics for 2019:\n",
      "   ðŸ“Š JPG: domains_2019.jpg\n",
      "   ðŸŒ HTML: domains_2019.html\n",
      "   ðŸ“ˆ Stats: stats_2019.json\n",
      "\n",
      "ðŸ”„ Processing 2020...\n",
      "Processing articles for 2020...\n",
      "âœ… Graph built for 2020:\n",
      "   ðŸ“Š 200 articles processed\n",
      "   ðŸ”¬ 96 unique domains found\n",
      "   ðŸ”— 1769 domain connections\n",
      "   ðŸŒ 200 multidisciplinary articles\n",
      "   ðŸŽ¯ Using threshold: 12.0 (showing 188/1769 connections)\n",
      "ðŸ’¾ Saved graph and statistics for 2020:\n",
      "   ðŸ“Š JPG: domains_2020.jpg\n",
      "   ðŸŒ HTML: domains_2020.html\n",
      "   ðŸ“ˆ Stats: stats_2020.json\n",
      "\n",
      "ðŸ”„ Processing 2021...\n",
      "Processing articles for 2021...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "  Processed 16000 articles...\n",
      "  Processed 17000 articles...\n",
      "  Processed 18000 articles...\n",
      "  Processed 19000 articles...\n",
      "  Processed 20000 articles...\n",
      "âœ… Graph built for 2021:\n",
      "   ðŸ“Š 20829 articles processed\n",
      "   ðŸ”¬ 122 unique domains found\n",
      "   ðŸ”— 5876 domain connections\n",
      "   ðŸŒ 20828 multidisciplinary articles\n",
      "   ðŸŽ¯ Using threshold: 401.5 (showing 588/5876 connections)\n",
      "ðŸ’¾ Saved graph and statistics for 2021:\n",
      "   ðŸ“Š JPG: domains_2021.jpg\n",
      "   ðŸŒ HTML: domains_2021.html\n",
      "   ðŸ“ˆ Stats: stats_2021.json\n",
      "\n",
      "ðŸ”„ Processing 2022...\n",
      "Processing articles for 2022...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "âœ… Graph built for 2022:\n",
      "   ðŸ“Š 15166 articles processed\n",
      "   ðŸ”¬ 119 unique domains found\n",
      "   ðŸ”— 5683 domain connections\n",
      "   ðŸŒ 15166 multidisciplinary articles\n",
      "   ðŸŽ¯ Using threshold: 294.60000000000036 (showing 569/5683 connections)\n",
      "ðŸ’¾ Saved graph and statistics for 2022:\n",
      "   ðŸ“Š JPG: domains_2022.jpg\n",
      "   ðŸŒ HTML: domains_2022.html\n",
      "   ðŸ“ˆ Stats: stats_2022.json\n",
      "\n",
      "ðŸ”„ Processing 2023...\n",
      "Processing articles for 2023...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "  Processed 16000 articles...\n",
      "âœ… Graph built for 2023:\n",
      "   ðŸ“Š 16927 articles processed\n",
      "   ðŸ”¬ 122 unique domains found\n",
      "   ðŸ”— 5805 domain connections\n",
      "   ðŸŒ 16927 multidisciplinary articles\n",
      "   ðŸŽ¯ Using threshold: 324.60000000000036 (showing 581/5805 connections)\n",
      "ðŸ’¾ Saved graph and statistics for 2023:\n",
      "   ðŸ“Š JPG: domains_2023.jpg\n",
      "   ðŸŒ HTML: domains_2023.html\n",
      "   ðŸ“ˆ Stats: stats_2023.json\n",
      "\n",
      "ðŸ”„ Processing 2024...\n",
      "Processing articles for 2024...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "  Processed 16000 articles...\n",
      "  Processed 17000 articles...\n",
      "  Processed 18000 articles...\n",
      "  Processed 19000 articles...\n",
      "  Processed 20000 articles...\n",
      "  Processed 21000 articles...\n",
      "âœ… Graph built for 2024:\n",
      "   ðŸ“Š 21405 articles processed\n",
      "   ðŸ”¬ 122 unique domains found\n",
      "   ðŸ”— 5881 domain connections\n",
      "   ðŸŒ 21403 multidisciplinary articles\n",
      "   ðŸŽ¯ Using threshold: 370.0 (showing 590/5881 connections)\n",
      "ðŸ’¾ Saved graph and statistics for 2024:\n",
      "   ðŸ“Š JPG: domains_2024.jpg\n",
      "   ðŸŒ HTML: domains_2024.html\n",
      "   ðŸ“ˆ Stats: stats_2024.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "# ========= Expanded domain list with better categorization =========\n",
    "KNOWN_DOMAINS = {\n",
    "    # STEM Core\n",
    "    \"Medicine\", \"Biology\", \"Chemistry\", \"Physics\", \"Mathematics\", \"Computer Science\",\n",
    "    \"Engineering\", \"Earth Science\", \"Environmental Science\", \"Materials Science\",\n",
    "    \"Astronomy\", \"Astrophysics\", \"Geology\", \"Meteorology\", \"Oceanography\",\n",
    "    \n",
    "    # Life Sciences\n",
    "    \"Genetics\", \"Molecular Biology\", \"Cell Biology\", \"Biochemistry\", \"Microbiology\",\n",
    "    \"Immunology\", \"Pharmacology\", \"Pathology\", \"Physiology\", \"Anatomy\",\n",
    "    \"Botany\", \"Zoology\", \"Ecology\", \"Evolutionary Biology\", \"Marine Biology\",\n",
    "    \n",
    "    # Medical & Health\n",
    "    \"Public Health\", \"Epidemiology\", \"Clinical Medicine\", \"Surgery\", \"Psychiatry\",\n",
    "    \"Neurology\", \"Cardiology\", \"Oncology\", \"Pediatrics\", \"Geriatrics\",\n",
    "    \"Radiology\", \"Anesthesiology\", \"Emergency Medicine\", \"Family Medicine\",\n",
    "    \n",
    "    # Technology & Computing\n",
    "    \"Artificial Intelligence\", \"Machine Learning\", \"Data Science\", \"Robotics\",\n",
    "    \"Software Engineering\", \"Cybersecurity\", \"Human-Computer Interaction\",\n",
    "    \"Information Systems\", \"Telecommunications\", \"Biotechnology\",\n",
    "    \"Nanotechnology\", \"Quantum Computing\", \"Bioinformatics\",\n",
    "    \n",
    "    # Engineering Disciplines\n",
    "    \"Mechanical Engineering\", \"Electrical Engineering\", \"Civil Engineering\",\n",
    "    \"Chemical Engineering\", \"Aerospace Engineering\", \"Biomedical Engineering\",\n",
    "    \"Environmental Engineering\", \"Industrial Engineering\", \"Nuclear Engineering\",\n",
    "    \n",
    "    # Social Sciences\n",
    "    \"Psychology\", \"Sociology\", \"Political Science\", \"Economics\", \"Anthropology\",\n",
    "    \"Education\", \"Criminology\", \"Social Work\", \"International Relations\",\n",
    "    \"Public Policy\", \"Urban Planning\", \"Communication\",\n",
    "    \n",
    "    # Humanities & Arts\n",
    "    \"History\", \"Philosophy\", \"Literature\", \"Art\", \"Music\", \"Theater\",\n",
    "    \"Linguistics\", \"Languages\", \"Cultural Studies\", \"Religious Studies\",\n",
    "    \"Media Studies\", \"Film Studies\",\n",
    "    \n",
    "    # Business & Management\n",
    "    \"Business\", \"Management\", \"Marketing\", \"Finance\", \"Accounting\",\n",
    "    \"Operations Research\", \"Supply Chain\", \"Entrepreneurship\",\n",
    "    \n",
    "    # Interdisciplinary\n",
    "    \"Neuroscience\", \"Cognitive Science\", \"Environmental Studies\", \n",
    "    \"Climate Science\", \"Sustainability\", \"Gender Studies\", \"Area Studies\",\n",
    "    \"Science and Technology Studies\", \"Bioethics\", \"Digital Humanities\",\n",
    "    \n",
    "    # Geography & Earth\n",
    "    \"Geography\", \"Cartography\", \"Geographic Information Systems\", \"Remote Sensing\",\n",
    "    \n",
    "    # Law & Policy\n",
    "    \"Law\", \"Legal Studies\", \"Constitutional Law\", \"International Law\",\n",
    "    \n",
    "    # Applied Sciences\n",
    "    \"Agriculture\", \"Forestry\", \"Veterinary Science\", \"Food Science\",\n",
    "    \"Sports Science\", \"Nutrition\", \"Architecture\", \"Design\"\n",
    "}\n",
    "\n",
    "# Convert to set for O(1) lookup\n",
    "DOMAIN_SET = set(KNOWN_DOMAINS)\n",
    "\n",
    "# ========= Create keyword mappings for faster lookup =========\n",
    "def create_domain_keywords():\n",
    "    \"\"\"Create keyword mappings to domains for faster matching\"\"\"\n",
    "    keyword_to_domain = {}\n",
    "    \n",
    "    # Direct mappings\n",
    "    for domain in DOMAIN_SET:\n",
    "        # Add the domain itself\n",
    "        keyword_to_domain[domain.lower()] = domain\n",
    "        \n",
    "        # Add common variations and keywords\n",
    "        words = domain.lower().split()\n",
    "        for word in words:\n",
    "            if len(word) > 3:  # Only meaningful words\n",
    "                keyword_to_domain[word] = domain\n",
    "    \n",
    "    # Add specific keyword mappings\n",
    "    additional_mappings = {\n",
    "        # Technology\n",
    "        \"ai\": \"Artificial Intelligence\", \"ml\": \"Machine Learning\",\n",
    "        \"deep learning\": \"Machine Learning\", \"neural network\": \"Machine Learning\",\n",
    "        \"algorithm\": \"Computer Science\", \"programming\": \"Computer Science\",\n",
    "        \"software\": \"Software Engineering\", \"hardware\": \"Engineering\",\n",
    "        \"database\": \"Computer Science\", \"network\": \"Telecommunications\",\n",
    "        \n",
    "        # Medicine\n",
    "        \"medical\": \"Medicine\", \"clinical\": \"Clinical Medicine\", \"patient\": \"Medicine\",\n",
    "        \"treatment\": \"Medicine\", \"diagnosis\": \"Medicine\", \"therapy\": \"Medicine\",\n",
    "        \"disease\": \"Medicine\", \"health\": \"Public Health\", \"healthcare\": \"Medicine\",\n",
    "        \"hospital\": \"Medicine\", \"nursing\": \"Medicine\", \"pharmaceutical\": \"Pharmacology\",\n",
    "        \n",
    "        # Biology\n",
    "        \"cell\": \"Cell Biology\", \"gene\": \"Genetics\", \"dna\": \"Genetics\", \"rna\": \"Genetics\",\n",
    "        \"protein\": \"Biochemistry\", \"enzyme\": \"Biochemistry\", \"organism\": \"Biology\",\n",
    "        \"species\": \"Biology\", \"evolution\": \"Evolutionary Biology\", \"genome\": \"Genetics\",\n",
    "        \n",
    "        # Physics/Chemistry\n",
    "        \"quantum\": \"Physics\", \"particle\": \"Physics\", \"energy\": \"Physics\",\n",
    "        \"molecule\": \"Chemistry\", \"reaction\": \"Chemistry\", \"catalyst\": \"Chemistry\",\n",
    "        \"material\": \"Materials Science\", \"crystal\": \"Materials Science\",\n",
    "        \n",
    "        # Social Sciences\n",
    "        \"social\": \"Sociology\", \"society\": \"Sociology\", \"culture\": \"Anthropology\",\n",
    "        \"behavior\": \"Psychology\", \"psychological\": \"Psychology\", \"cognitive\": \"Psychology\",\n",
    "        \"economic\": \"Economics\", \"political\": \"Political Science\", \"policy\": \"Public Policy\",\n",
    "        \"education\": \"Education\", \"learning\": \"Education\", \"teaching\": \"Education\",\n",
    "        \n",
    "        # Environment\n",
    "        \"climate\": \"Climate Science\", \"environment\": \"Environmental Science\",\n",
    "        \"sustainability\": \"Sustainability\", \"ecology\": \"Ecology\", \"conservation\": \"Environmental Science\",\n",
    "        \"pollution\": \"Environmental Science\", \"renewable\": \"Environmental Science\",\n",
    "        \n",
    "        # Business\n",
    "        \"business\": \"Business\", \"management\": \"Management\", \"marketing\": \"Marketing\",\n",
    "        \"finance\": \"Finance\", \"economic\": \"Economics\", \"market\": \"Economics\",\n",
    "        \n",
    "        # Geography\n",
    "        \"geographic\": \"Geography\", \"spatial\": \"Geography\", \"mapping\": \"Geography\",\n",
    "        \"urban\": \"Urban Planning\", \"city\": \"Urban Planning\"\n",
    "    }\n",
    "    \n",
    "    keyword_to_domain.update(additional_mappings)\n",
    "    return keyword_to_domain\n",
    "\n",
    "# ========= Fast domain extraction =========\n",
    "def extract_domains_from_article_fast(article, keyword_to_domain):\n",
    "    \"\"\"\n",
    "    Fast domain extraction using keyword matching instead of fuzzy string matching\n",
    "    \"\"\"\n",
    "    found_domains = set()\n",
    "    \n",
    "    # Collect all text to search\n",
    "    text_sources = []\n",
    "    text_sources.extend(article.get(\"domains\", []))\n",
    "    text_sources.extend(article.get(\"fields\", []))\n",
    "    text_sources.extend(article.get(\"keywords\", []))\n",
    "    \n",
    "    # Also check title for domain keywords\n",
    "    title = article.get(\"title\", \"\")\n",
    "    if title:\n",
    "        text_sources.append(title)\n",
    "    \n",
    "    # Search for domain matches\n",
    "    for text in text_sources:\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Direct domain match\n",
    "        if text_lower in keyword_to_domain:\n",
    "            found_domains.add(keyword_to_domain[text_lower])\n",
    "            continue\n",
    "        \n",
    "        # Check if any keyword is contained in the text\n",
    "        for keyword, domain in keyword_to_domain.items():\n",
    "            if keyword in text_lower:\n",
    "                found_domains.add(domain)\n",
    "    \n",
    "    return list(found_domains)\n",
    "\n",
    "# ========= Enhanced graph building with statistics =========\n",
    "def build_domain_graph_fast(year, data_folder=\"articles_{year}_new\"):\n",
    "    \"\"\"Build domain graph with improved performance and statistics\"\"\"\n",
    "    file_path = os.path.join(data_folder.format(year=year), \"all_articles_enhanced.jsonl\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš  No data for {year}\")\n",
    "        return None, {}\n",
    "\n",
    "    keyword_to_domain = create_domain_keywords()\n",
    "    G = nx.Graph()\n",
    "    domain_pairs_counter = defaultdict(int)\n",
    "    domain_article_count = defaultdict(int)\n",
    "    total_articles = 0\n",
    "    multidisciplinary_articles = 0\n",
    "\n",
    "    print(f\"Processing articles for {year}...\")\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if line_num % 1000 == 0:\n",
    "                print(f\"  Processed {line_num} articles...\")\n",
    "                \n",
    "            try:\n",
    "                article = json.loads(line)\n",
    "                total_articles += 1\n",
    "                \n",
    "                domains = extract_domains_from_article_fast(article, keyword_to_domain)\n",
    "                \n",
    "                # Count domain occurrences\n",
    "                for domain in domains:\n",
    "                    domain_article_count[domain] += 1\n",
    "                \n",
    "                if len(domains) > 1:\n",
    "                    multidisciplinary_articles += 1\n",
    "                    # Add all possible pairs\n",
    "                    for d1, d2 in itertools.combinations(sorted(domains), 2):\n",
    "                        domain_pairs_counter[(d1, d2)] += 1\n",
    "                        \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"  Warning: Skipped malformed JSON at line {line_num}\")\n",
    "                continue\n",
    "\n",
    "    # Build graph with edge weights\n",
    "    for (d1, d2), weight in domain_pairs_counter.items():\n",
    "        G.add_edge(d1, d2, weight=weight)\n",
    "\n",
    "    # Add isolated nodes (domains that appear but don't have connections)\n",
    "    for domain in domain_article_count:\n",
    "        if domain not in G.nodes():\n",
    "            G.add_node(domain)\n",
    "\n",
    "    # Calculate statistics\n",
    "    stats = {\n",
    "        \"total_articles\": total_articles,\n",
    "        \"multidisciplinary_articles\": multidisciplinary_articles,\n",
    "        \"total_domains\": len(domain_article_count),\n",
    "        \"connected_domains\": len([n for n in G.nodes() if G.degree(n) > 0]),\n",
    "        \"total_connections\": len(G.edges()),\n",
    "        \"top_domains\": sorted(domain_article_count.items(), key=lambda x: x[1], reverse=True)[:10],\n",
    "        \"strongest_connections\": sorted(domain_pairs_counter.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    }\n",
    "\n",
    "    print(f\"âœ… Graph built for {year}:\")\n",
    "    print(f\"   ðŸ“Š {stats['total_articles']} articles processed\")\n",
    "    print(f\"   ðŸ”¬ {stats['total_domains']} unique domains found\")\n",
    "    print(f\"   ðŸ”— {stats['total_connections']} domain connections\")\n",
    "    print(f\"   ðŸŒ {stats['multidisciplinary_articles']} multidisciplinary articles\")\n",
    "\n",
    "    return G, stats\n",
    "\n",
    "# ========= Enhanced visualization =========\n",
    "def save_graph_enhanced(G, year, stats, output_dir=\"domain_graphs\"):\n",
    "    \"\"\"Save graph with enhanced visualizations and statistics\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if len(G.nodes()) == 0:\n",
    "        print(f\"âš  No nodes to visualize for {year}\")\n",
    "        return\n",
    "\n",
    "    # Filter graph to show only meaningful connections\n",
    "    # Calculate better thresholds based on data distribution\n",
    "    edge_weights = [data['weight'] for u, v, data in G.edges(data=True)]\n",
    "    if len(edge_weights) > 0:\n",
    "        # Use 75th percentile as threshold to show only strong connections\n",
    "        import numpy as np\n",
    "        threshold_75 = np.percentile(edge_weights, 75)\n",
    "        threshold_90 = np.percentile(edge_weights, 90)\n",
    "        \n",
    "        # For very dense networks, use 90th percentile\n",
    "        min_weight = threshold_90 if len(G.edges()) > 1000 else threshold_75\n",
    "        min_weight = max(min_weight, 2)  # At least 2 collaborations\n",
    "    else:\n",
    "        min_weight = 1\n",
    "    \n",
    "    G_filtered = nx.Graph()\n",
    "    \n",
    "    for u, v, data in G.edges(data=True):\n",
    "        if data['weight'] >= min_weight:\n",
    "            G_filtered.add_edge(u, v, weight=data['weight'])\n",
    "    \n",
    "    # If filtered graph is too small, lower the threshold\n",
    "    if len(G_filtered.nodes()) < 20:\n",
    "        min_weight = max(1, min_weight // 2)\n",
    "        G_filtered = nx.Graph()\n",
    "        for u, v, data in G.edges(data=True):\n",
    "            if data['weight'] >= min_weight:\n",
    "                G_filtered.add_edge(u, v, weight=data['weight'])\n",
    "    \n",
    "    print(f\"   ðŸŽ¯ Using threshold: {min_weight} (showing {len(G_filtered.edges())}/{len(G.edges())} connections)\")\n",
    "\n",
    "    # Better layout for readability\n",
    "    pos = nx.spring_layout(G_filtered, seed=42, k=2.0, iterations=100)\n",
    "\n",
    "    # === MATPLOTLIB VISUALIZATION (JPG) ===\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Calculate node sizes based on degree (bigger for more connections)\n",
    "    node_sizes = [G_filtered.degree(n) * 150 + 500 for n in G_filtered.nodes()]\n",
    "    \n",
    "    # Calculate edge widths based on collaboration strength\n",
    "    edge_weights = [G_filtered[u][v]['weight'] for u, v in G_filtered.edges()]\n",
    "    max_weight = max(edge_weights) if edge_weights else 1\n",
    "    edge_widths = [w/max_weight * 6 + 1 for w in edge_weights]\n",
    "\n",
    "    # Draw edges first (behind nodes)\n",
    "    nx.draw_networkx_edges(G_filtered, pos, width=edge_widths, \n",
    "                          edge_color=\"lightgray\", alpha=0.4)\n",
    "    \n",
    "    # Draw nodes with better colors\n",
    "    nx.draw_networkx_nodes(G_filtered, pos, node_size=node_sizes, \n",
    "                          node_color=[G_filtered.degree(n) for n in G_filtered.nodes()], \n",
    "                          cmap=plt.cm.viridis, alpha=0.8, \n",
    "                          edgecolors='white', linewidths=2)\n",
    "    \n",
    "    # Draw labels with better positioning\n",
    "    labels = {}\n",
    "    for node in G_filtered.nodes():\n",
    "        # Shorten long domain names for readability\n",
    "        label = node.replace(' and ', ' & ').replace('Science', 'Sci.')\n",
    "        if len(label) > 15:\n",
    "            words = label.split()\n",
    "            if len(words) > 1:\n",
    "                label = words[0] + '\\n' + ' '.join(words[1:])\n",
    "        labels[node] = label\n",
    "    \n",
    "    nx.draw_networkx_labels(G_filtered, pos, labels, font_size=9, \n",
    "                           font_weight='bold', font_color='white')\n",
    "    \n",
    "    plt.title(f\"Domain Collaboration Network {year}\\n\" +\n",
    "              f\"{stats['total_articles']:,} articles â€¢ {len(G_filtered.nodes())} domains shown â€¢ \" +\n",
    "              f\"Min {min_weight} collaborations per connection\", \n",
    "              fontsize=18, pad=30)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"domains_{year}.jpg\"), \n",
    "                dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.close()\n",
    "\n",
    "    # === PLOTLY VISUALIZATION (HTML) ===\n",
    "    # Prepare edge traces\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    edge_info = []\n",
    "    \n",
    "    for u, v in G_filtered.edges():\n",
    "        x0, y0 = pos[u]\n",
    "        x1, y1 = pos[v]\n",
    "        edge_x += [x0, x1, None]\n",
    "        edge_y += [y0, y1, None]\n",
    "        weight = G_filtered[u][v]['weight']\n",
    "        edge_info.append(f\"{u} â†” {v}<br>Collaborations: {weight}\")\n",
    "\n",
    "    edge_trace = go.Scatter(\n",
    "        x=edge_x, y=edge_y,\n",
    "        line=dict(width=0.8, color='rgba(125, 125, 125, 0.5)'),\n",
    "        hoverinfo='none',\n",
    "        mode='lines'\n",
    "    )\n",
    "\n",
    "    # Prepare node traces\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    node_text = []\n",
    "    node_info = []\n",
    "    node_sizes = []\n",
    "    node_colors = []\n",
    "    \n",
    "    for i, node in enumerate(G_filtered.nodes()):\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        \n",
    "        # Calculate metrics for this node\n",
    "        degree = G_filtered.degree(node)\n",
    "        neighbors = list(G_filtered.neighbors(node))\n",
    "        \n",
    "        # Node size based on degree (bigger for more connections)\n",
    "        size = min(degree * 12 + 20, 80)  # Cap maximum size\n",
    "        node_sizes.append(size)\n",
    "        node_colors.append(degree)\n",
    "        \n",
    "        # Shorter node labels for readability\n",
    "        short_name = node.replace(' and ', ' & ').replace('Science', 'Sci.')\n",
    "        if len(short_name) > 12:\n",
    "            words = short_name.split()\n",
    "            if len(words) > 1:\n",
    "                short_name = words[0] + '<br>' + ' '.join(words[1:])\n",
    "        node_text.append(short_name)\n",
    "        \n",
    "        # Enhanced hover info with collaboration strengths\n",
    "        neighbor_info = []\n",
    "        for neighbor in neighbors:\n",
    "            weight = G_filtered[node][neighbor]['weight']\n",
    "            neighbor_info.append(f\"â€¢ {neighbor}: {weight} articles\")\n",
    "        \n",
    "        neighbor_text = \"<br>\".join(neighbor_info[:8])  # Show max 8 neighbors\n",
    "        if len(neighbors) > 8:\n",
    "            neighbor_text += f\"<br>... and {len(neighbors)-8} more\"\n",
    "            \n",
    "        hover_text = f\"<b>{node}</b><br>\" \\\n",
    "                    f\"<b>Total Connections:</b> {degree}<br>\" \\\n",
    "                    f\"<b>Collaboration Details:</b><br>{neighbor_text}\"\n",
    "        node_info.append(hover_text)\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y,\n",
    "        mode='markers+text',\n",
    "        text=node_text,\n",
    "        textposition=\"middle center\",\n",
    "        textfont=dict(size=8, color=\"white\", family=\"Arial Black\"),\n",
    "        hovertext=node_info,\n",
    "        hoverinfo='text',\n",
    "        marker=dict(\n",
    "            size=node_sizes,\n",
    "            color=node_colors,\n",
    "            # colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title=dict(\n",
    "                    text=\"Connections\",\n",
    "                    side=\"right\"\n",
    "                ),\n",
    "                tickmode=\"linear\",\n",
    "                thickness=12,\n",
    "                len=0.7\n",
    "            ),\n",
    "            line=dict(width=3, color='white'),\n",
    "            opacity=0.9\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the figure with better layout\n",
    "    fig = go.Figure(data=[edge_trace, node_trace],\n",
    "                    layout=go.Layout(\n",
    "                        title=dict(\n",
    "                            text=f\"<b>Domain Collaboration Network {year}</b><br>\" +\n",
    "                                 f\"<i>{stats['total_articles']:,} articles â€¢ {len(G_filtered.nodes())} domains shown â€¢ \" +\n",
    "                                 f\"Min {min_weight} collaborations â€¢ Threshold: {min_weight}+ articles</i>\",\n",
    "                            x=0.5,\n",
    "                            font=dict(size=16)\n",
    "                        ),\n",
    "                        showlegend=False,\n",
    "                        hovermode='closest',\n",
    "                        margin=dict(b=40, l=20, r=60, t=120),\n",
    "                        annotations=[\n",
    "                            dict(\n",
    "                                text=\"<b>Guide:</b> Node size = # connections | Edge thickness = collaboration strength<br>\" +\n",
    "                                     f\"Only showing connections with {min_weight}+ shared articles | Hover for details\",\n",
    "                                showarrow=False,\n",
    "                                xref=\"paper\", yref=\"paper\",\n",
    "                                x=0.5, y=-0.08,\n",
    "                                xanchor='center', yanchor='top',\n",
    "                                font=dict(size=11, color=\"gray\")\n",
    "                            )\n",
    "                        ],\n",
    "                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                        plot_bgcolor='white',\n",
    "                        paper_bgcolor='white',\n",
    "                        width=1200,\n",
    "                        height=800\n",
    "                    ))\n",
    "\n",
    "    # Save HTML\n",
    "    html_file = os.path.join(output_dir, f\"domains_{year}.html\")\n",
    "    fig.write_html(html_file)\n",
    "\n",
    "    # Save statistics\n",
    "    with open(os.path.join(output_dir, f\"stats_{year}.json\"), \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "\n",
    "    print(f\"ðŸ’¾ Saved graph and statistics for {year}:\")\n",
    "    print(f\"   ðŸ“Š JPG: domains_{year}.jpg\")\n",
    "    print(f\"   ðŸŒ HTML: domains_{year}.html\") \n",
    "    print(f\"   ðŸ“ˆ Stats: stats_{year}.json\")\n",
    "\n",
    "# ========= Main execution =========\n",
    "def main():\n",
    "    years = [2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"\\nðŸ”„ Processing {year}...\")\n",
    "        G, stats = build_domain_graph_fast(year)\n",
    "        \n",
    "        if G and len(G.nodes()) > 0:\n",
    "            save_graph_enhanced(G, year, stats)\n",
    "        else:\n",
    "            print(f\"âš  No valid graph generated for {year}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2928aef0-288a-45aa-aa9c-efffbb6aa49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Processing 2018...\n",
      "Processing articles for 2018...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "âœ… Graph built for 2018:\n",
      "   ðŸ“Š 15797 articles processed\n",
      "   ðŸ”¬ 122 unique domains found\n",
      "   ðŸ”— 5721 domain connections\n",
      "   ðŸŒ 15797 multidisciplinary articles\n",
      "ðŸ’¾ Created multiple visualizations for 2018:\n",
      "   ðŸ“Š Top domains: top_domains_2018.html\n",
      "   ðŸ¤ Collaborations: top_collaborations_2018.html\n",
      "   ðŸ·ï¸  Categories: category_network_2018.html\n",
      "   ðŸŽ¯ Focused network: focused_network_2018.html\n",
      "   ðŸ“ˆ Stats: stats_2018.json\n",
      "\n",
      "ðŸ”„ Processing 2019...\n",
      "Processing articles for 2019...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "âœ… Graph built for 2019:\n",
      "   ðŸ“Š 15612 articles processed\n",
      "   ðŸ”¬ 122 unique domains found\n",
      "   ðŸ”— 5744 domain connections\n",
      "   ðŸŒ 15612 multidisciplinary articles\n",
      "ðŸ’¾ Created multiple visualizations for 2019:\n",
      "   ðŸ“Š Top domains: top_domains_2019.html\n",
      "   ðŸ¤ Collaborations: top_collaborations_2019.html\n",
      "   ðŸ·ï¸  Categories: category_network_2019.html\n",
      "   ðŸŽ¯ Focused network: focused_network_2019.html\n",
      "   ðŸ“ˆ Stats: stats_2019.json\n",
      "\n",
      "ðŸ”„ Processing 2020...\n",
      "Processing articles for 2020...\n",
      "âœ… Graph built for 2020:\n",
      "   ðŸ“Š 200 articles processed\n",
      "   ðŸ”¬ 96 unique domains found\n",
      "   ðŸ”— 1769 domain connections\n",
      "   ðŸŒ 200 multidisciplinary articles\n",
      "ðŸ’¾ Created multiple visualizations for 2020:\n",
      "   ðŸ“Š Top domains: top_domains_2020.html\n",
      "   ðŸ¤ Collaborations: top_collaborations_2020.html\n",
      "   ðŸ·ï¸  Categories: category_network_2020.html\n",
      "   ðŸŽ¯ Focused network: focused_network_2020.html\n",
      "   ðŸ“ˆ Stats: stats_2020.json\n",
      "\n",
      "ðŸ”„ Processing 2021...\n",
      "Processing articles for 2021...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "  Processed 16000 articles...\n",
      "  Processed 17000 articles...\n",
      "  Processed 18000 articles...\n",
      "  Processed 19000 articles...\n",
      "  Processed 20000 articles...\n",
      "âœ… Graph built for 2021:\n",
      "   ðŸ“Š 20829 articles processed\n",
      "   ðŸ”¬ 122 unique domains found\n",
      "   ðŸ”— 5876 domain connections\n",
      "   ðŸŒ 20828 multidisciplinary articles\n",
      "ðŸ’¾ Created multiple visualizations for 2021:\n",
      "   ðŸ“Š Top domains: top_domains_2021.html\n",
      "   ðŸ¤ Collaborations: top_collaborations_2021.html\n",
      "   ðŸ·ï¸  Categories: category_network_2021.html\n",
      "   ðŸŽ¯ Focused network: focused_network_2021.html\n",
      "   ðŸ“ˆ Stats: stats_2021.json\n",
      "\n",
      "ðŸ”„ Processing 2022...\n",
      "Processing articles for 2022...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "âœ… Graph built for 2022:\n",
      "   ðŸ“Š 15166 articles processed\n",
      "   ðŸ”¬ 119 unique domains found\n",
      "   ðŸ”— 5683 domain connections\n",
      "   ðŸŒ 15166 multidisciplinary articles\n",
      "ðŸ’¾ Created multiple visualizations for 2022:\n",
      "   ðŸ“Š Top domains: top_domains_2022.html\n",
      "   ðŸ¤ Collaborations: top_collaborations_2022.html\n",
      "   ðŸ·ï¸  Categories: category_network_2022.html\n",
      "   ðŸŽ¯ Focused network: focused_network_2022.html\n",
      "   ðŸ“ˆ Stats: stats_2022.json\n",
      "\n",
      "ðŸ”„ Processing 2023...\n",
      "Processing articles for 2023...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "  Processed 16000 articles...\n",
      "âœ… Graph built for 2023:\n",
      "   ðŸ“Š 16927 articles processed\n",
      "   ðŸ”¬ 122 unique domains found\n",
      "   ðŸ”— 5805 domain connections\n",
      "   ðŸŒ 16927 multidisciplinary articles\n",
      "ðŸ’¾ Created multiple visualizations for 2023:\n",
      "   ðŸ“Š Top domains: top_domains_2023.html\n",
      "   ðŸ¤ Collaborations: top_collaborations_2023.html\n",
      "   ðŸ·ï¸  Categories: category_network_2023.html\n",
      "   ðŸŽ¯ Focused network: focused_network_2023.html\n",
      "   ðŸ“ˆ Stats: stats_2023.json\n",
      "\n",
      "ðŸ”„ Processing 2024...\n",
      "Processing articles for 2024...\n",
      "  Processed 1000 articles...\n",
      "  Processed 2000 articles...\n",
      "  Processed 3000 articles...\n",
      "  Processed 4000 articles...\n",
      "  Processed 5000 articles...\n",
      "  Processed 6000 articles...\n",
      "  Processed 7000 articles...\n",
      "  Processed 8000 articles...\n",
      "  Processed 9000 articles...\n",
      "  Processed 10000 articles...\n",
      "  Processed 11000 articles...\n",
      "  Processed 12000 articles...\n",
      "  Processed 13000 articles...\n",
      "  Processed 14000 articles...\n",
      "  Processed 15000 articles...\n",
      "  Processed 16000 articles...\n",
      "  Processed 17000 articles...\n",
      "  Processed 18000 articles...\n",
      "  Processed 19000 articles...\n",
      "  Processed 20000 articles...\n",
      "  Processed 21000 articles...\n",
      "âœ… Graph built for 2024:\n",
      "   ðŸ“Š 21405 articles processed\n",
      "   ðŸ”¬ 122 unique domains found\n",
      "   ðŸ”— 5881 domain connections\n",
      "   ðŸŒ 21403 multidisciplinary articles\n",
      "ðŸ’¾ Created multiple visualizations for 2024:\n",
      "   ðŸ“Š Top domains: top_domains_2024.html\n",
      "   ðŸ¤ Collaborations: top_collaborations_2024.html\n",
      "   ðŸ·ï¸  Categories: category_network_2024.html\n",
      "   ðŸŽ¯ Focused network: focused_network_2024.html\n",
      "   ðŸ“ˆ Stats: stats_2024.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import pandas as pd\n",
    "\n",
    "# ========= Expanded domain list with better categorization =========\n",
    "KNOWN_DOMAINS = {\n",
    "    # STEM Core\n",
    "    \"Medicine\", \"Biology\", \"Chemistry\", \"Physics\", \"Mathematics\", \"Computer Science\",\n",
    "    \"Engineering\", \"Earth Science\", \"Environmental Science\", \"Materials Science\",\n",
    "    \"Astronomy\", \"Astrophysics\", \"Geology\", \"Meteorology\", \"Oceanography\",\n",
    "    \n",
    "    # Life Sciences\n",
    "    \"Genetics\", \"Molecular Biology\", \"Cell Biology\", \"Biochemistry\", \"Microbiology\",\n",
    "    \"Immunology\", \"Pharmacology\", \"Pathology\", \"Physiology\", \"Anatomy\",\n",
    "    \"Botany\", \"Zoology\", \"Ecology\", \"Evolutionary Biology\", \"Marine Biology\",\n",
    "    \n",
    "    # Medical & Health\n",
    "    \"Public Health\", \"Epidemiology\", \"Clinical Medicine\", \"Surgery\", \"Psychiatry\",\n",
    "    \"Neurology\", \"Cardiology\", \"Oncology\", \"Pediatrics\", \"Geriatrics\",\n",
    "    \"Radiology\", \"Anesthesiology\", \"Emergency Medicine\", \"Family Medicine\",\n",
    "    \n",
    "    # Technology & Computing\n",
    "    \"Artificial Intelligence\", \"Machine Learning\", \"Data Science\", \"Robotics\",\n",
    "    \"Software Engineering\", \"Cybersecurity\", \"Human-Computer Interaction\",\n",
    "    \"Information Systems\", \"Telecommunications\", \"Biotechnology\",\n",
    "    \"Nanotechnology\", \"Quantum Computing\", \"Bioinformatics\",\n",
    "    \n",
    "    # Engineering Disciplines\n",
    "    \"Mechanical Engineering\", \"Electrical Engineering\", \"Civil Engineering\",\n",
    "    \"Chemical Engineering\", \"Aerospace Engineering\", \"Biomedical Engineering\",\n",
    "    \"Environmental Engineering\", \"Industrial Engineering\", \"Nuclear Engineering\",\n",
    "    \n",
    "    # Social Sciences\n",
    "    \"Psychology\", \"Sociology\", \"Political Science\", \"Economics\", \"Anthropology\",\n",
    "    \"Education\", \"Criminology\", \"Social Work\", \"International Relations\",\n",
    "    \"Public Policy\", \"Urban Planning\", \"Communication\",\n",
    "    \n",
    "    # Humanities & Arts\n",
    "    \"History\", \"Philosophy\", \"Literature\", \"Art\", \"Music\", \"Theater\",\n",
    "    \"Linguistics\", \"Languages\", \"Cultural Studies\", \"Religious Studies\",\n",
    "    \"Media Studies\", \"Film Studies\",\n",
    "    \n",
    "    # Business & Management\n",
    "    \"Business\", \"Management\", \"Marketing\", \"Finance\", \"Accounting\",\n",
    "    \"Operations Research\", \"Supply Chain\", \"Entrepreneurship\",\n",
    "    \n",
    "    # Interdisciplinary\n",
    "    \"Neuroscience\", \"Cognitive Science\", \"Environmental Studies\", \n",
    "    \"Climate Science\", \"Sustainability\", \"Gender Studies\", \"Area Studies\",\n",
    "    \"Science and Technology Studies\", \"Bioethics\", \"Digital Humanities\",\n",
    "    \n",
    "    # Geography & Earth\n",
    "    \"Geography\", \"Cartography\", \"Geographic Information Systems\", \"Remote Sensing\",\n",
    "    \n",
    "    # Law & Policy\n",
    "    \"Law\", \"Legal Studies\", \"Constitutional Law\", \"International Law\",\n",
    "    \n",
    "    # Applied Sciences\n",
    "    \"Agriculture\", \"Forestry\", \"Veterinary Science\", \"Food Science\",\n",
    "    \"Sports Science\", \"Nutrition\", \"Architecture\", \"Design\"\n",
    "}\n",
    "\n",
    "DOMAIN_SET = set(KNOWN_DOMAINS)\n",
    "\n",
    "def create_domain_keywords():\n",
    "    \"\"\"Create keyword mappings to domains for faster matching\"\"\"\n",
    "    keyword_to_domain = {}\n",
    "    \n",
    "    for domain in DOMAIN_SET:\n",
    "        keyword_to_domain[domain.lower()] = domain\n",
    "        words = domain.lower().split()\n",
    "        for word in words:\n",
    "            if len(word) > 3:\n",
    "                keyword_to_domain[word] = domain\n",
    "    \n",
    "    additional_mappings = {\n",
    "        # Technology\n",
    "        \"ai\": \"Artificial Intelligence\", \"ml\": \"Machine Learning\",\n",
    "        \"deep learning\": \"Machine Learning\", \"neural network\": \"Machine Learning\",\n",
    "        \"algorithm\": \"Computer Science\", \"programming\": \"Computer Science\",\n",
    "        \"software\": \"Software Engineering\", \"hardware\": \"Engineering\",\n",
    "        \"database\": \"Computer Science\", \"network\": \"Telecommunications\",\n",
    "        \n",
    "        # Medicine\n",
    "        \"medical\": \"Medicine\", \"clinical\": \"Clinical Medicine\", \"patient\": \"Medicine\",\n",
    "        \"treatment\": \"Medicine\", \"diagnosis\": \"Medicine\", \"therapy\": \"Medicine\",\n",
    "        \"disease\": \"Medicine\", \"health\": \"Public Health\", \"healthcare\": \"Medicine\",\n",
    "        \"hospital\": \"Medicine\", \"nursing\": \"Medicine\", \"pharmaceutical\": \"Pharmacology\",\n",
    "        \n",
    "        # Biology\n",
    "        \"cell\": \"Cell Biology\", \"gene\": \"Genetics\", \"dna\": \"Genetics\", \"rna\": \"Genetics\",\n",
    "        \"protein\": \"Biochemistry\", \"enzyme\": \"Biochemistry\", \"organism\": \"Biology\",\n",
    "        \"species\": \"Biology\", \"evolution\": \"Evolutionary Biology\", \"genome\": \"Genetics\",\n",
    "        \n",
    "        # Physics/Chemistry\n",
    "        \"quantum\": \"Physics\", \"particle\": \"Physics\", \"energy\": \"Physics\",\n",
    "        \"molecule\": \"Chemistry\", \"reaction\": \"Chemistry\", \"catalyst\": \"Chemistry\",\n",
    "        \"material\": \"Materials Science\", \"crystal\": \"Materials Science\",\n",
    "        \n",
    "        # Social Sciences\n",
    "        \"social\": \"Sociology\", \"society\": \"Sociology\", \"culture\": \"Anthropology\",\n",
    "        \"behavior\": \"Psychology\", \"psychological\": \"Psychology\", \"cognitive\": \"Psychology\",\n",
    "        \"economic\": \"Economics\", \"political\": \"Political Science\", \"policy\": \"Public Policy\",\n",
    "        \"education\": \"Education\", \"learning\": \"Education\", \"teaching\": \"Education\",\n",
    "        \n",
    "        # Environment\n",
    "        \"climate\": \"Climate Science\", \"environment\": \"Environmental Science\",\n",
    "        \"sustainability\": \"Sustainability\", \"ecology\": \"Ecology\", \"conservation\": \"Environmental Science\",\n",
    "        \"pollution\": \"Environmental Science\", \"renewable\": \"Environmental Science\",\n",
    "        \n",
    "        # Business\n",
    "        \"business\": \"Business\", \"management\": \"Management\", \"marketing\": \"Marketing\",\n",
    "        \"finance\": \"Finance\", \"economic\": \"Economics\", \"market\": \"Economics\",\n",
    "        \n",
    "        # Geography\n",
    "        \"geographic\": \"Geography\", \"spatial\": \"Geography\", \"mapping\": \"Geography\",\n",
    "        \"urban\": \"Urban Planning\", \"city\": \"Urban Planning\"\n",
    "    }\n",
    "    \n",
    "    keyword_to_domain.update(additional_mappings)\n",
    "    return keyword_to_domain\n",
    "\n",
    "def extract_domains_from_article_fast(article, keyword_to_domain):\n",
    "    \"\"\"Fast domain extraction using keyword matching\"\"\"\n",
    "    found_domains = set()\n",
    "    \n",
    "    text_sources = []\n",
    "    text_sources.extend(article.get(\"domains\", []))\n",
    "    text_sources.extend(article.get(\"fields\", []))\n",
    "    text_sources.extend(article.get(\"keywords\", []))\n",
    "    \n",
    "    title = article.get(\"title\", \"\")\n",
    "    if title:\n",
    "        text_sources.append(title)\n",
    "    \n",
    "    for text in text_sources:\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        if text_lower in keyword_to_domain:\n",
    "            found_domains.add(keyword_to_domain[text_lower])\n",
    "            continue\n",
    "        \n",
    "        for keyword, domain in keyword_to_domain.items():\n",
    "            if keyword in text_lower:\n",
    "                found_domains.add(domain)\n",
    "    \n",
    "    return list(found_domains)\n",
    "\n",
    "def build_domain_graph_fast(year, data_folder=\"articles_{year}_new\"):\n",
    "    \"\"\"Build domain graph with improved performance and statistics\"\"\"\n",
    "    file_path = os.path.join(data_folder.format(year=year), \"all_articles_enhanced.jsonl\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš  No data for {year}\")\n",
    "        return None, {}\n",
    "\n",
    "    keyword_to_domain = create_domain_keywords()\n",
    "    G = nx.Graph()\n",
    "    domain_pairs_counter = defaultdict(int)\n",
    "    domain_article_count = defaultdict(int)\n",
    "    total_articles = 0\n",
    "    multidisciplinary_articles = 0\n",
    "\n",
    "    print(f\"Processing articles for {year}...\")\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if line_num % 1000 == 0:\n",
    "                print(f\"  Processed {line_num} articles...\")\n",
    "                \n",
    "            try:\n",
    "                article = json.loads(line)\n",
    "                total_articles += 1\n",
    "                \n",
    "                domains = extract_domains_from_article_fast(article, keyword_to_domain)\n",
    "                \n",
    "                for domain in domains:\n",
    "                    domain_article_count[domain] += 1\n",
    "                \n",
    "                if len(domains) > 1:\n",
    "                    multidisciplinary_articles += 1\n",
    "                    for d1, d2 in itertools.combinations(sorted(domains), 2):\n",
    "                        domain_pairs_counter[(d1, d2)] += 1\n",
    "                        \n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"  Warning: Skipped malformed JSON at line {line_num}\")\n",
    "                continue\n",
    "\n",
    "    for (d1, d2), weight in domain_pairs_counter.items():\n",
    "        G.add_edge(d1, d2, weight=weight)\n",
    "\n",
    "    for domain in domain_article_count:\n",
    "        if domain not in G.nodes():\n",
    "            G.add_node(domain)\n",
    "\n",
    "    stats = {\n",
    "        \"total_articles\": total_articles,\n",
    "        \"multidisciplinary_articles\": multidisciplinary_articles,\n",
    "        \"total_domains\": len(domain_article_count),\n",
    "        \"connected_domains\": len([n for n in G.nodes() if G.degree(n) > 0]),\n",
    "        \"total_connections\": len(G.edges()),\n",
    "        \"top_domains\": sorted(domain_article_count.items(), key=lambda x: x[1], reverse=True)[:15],\n",
    "        \"strongest_connections\": sorted(domain_pairs_counter.items(), key=lambda x: x[1], reverse=True)[:15],\n",
    "        \"domain_article_count\": domain_article_count\n",
    "    }\n",
    "\n",
    "    print(f\"âœ… Graph built for {year}:\")\n",
    "    print(f\"   ðŸ“Š {stats['total_articles']} articles processed\")\n",
    "    print(f\"   ðŸ”¬ {stats['total_domains']} unique domains found\")\n",
    "    print(f\"   ðŸ”— {stats['total_connections']} domain connections\")\n",
    "    print(f\"   ðŸŒ {stats['multidisciplinary_articles']} multidisciplinary articles\")\n",
    "\n",
    "    return G, stats\n",
    "\n",
    "def create_domain_clusters(G, stats):\n",
    "    \"\"\"Create meaningful domain clusters\"\"\"\n",
    "    \n",
    "    # Define domain categories manually for better interpretation\n",
    "    domain_categories = {\n",
    "        \"Life Sciences\": [\"Biology\", \"Genetics\", \"Molecular Biology\", \"Cell Biology\", \"Biochemistry\", \n",
    "                         \"Microbiology\", \"Immunology\", \"Botany\", \"Zoology\", \"Ecology\", \"Evolutionary Biology\", \n",
    "                         \"Marine Biology\", \"Physiology\", \"Anatomy\"],\n",
    "        \n",
    "        \"Medicine & Health\": [\"Medicine\", \"Clinical Medicine\", \"Public Health\", \"Epidemiology\", \"Surgery\", \n",
    "                             \"Psychiatry\", \"Neurology\", \"Cardiology\", \"Oncology\", \"Pediatrics\", \"Geriatrics\",\n",
    "                             \"Radiology\", \"Anesthesiology\", \"Emergency Medicine\", \"Family Medicine\", \n",
    "                             \"Pharmacology\", \"Pathology\"],\n",
    "        \n",
    "        \"Technology & Computing\": [\"Computer Science\", \"Artificial Intelligence\", \"Machine Learning\", \n",
    "                                  \"Data Science\", \"Software Engineering\", \"Robotics\", \"Cybersecurity\",\n",
    "                                  \"Human-Computer Interaction\", \"Information Systems\", \"Telecommunications\",\n",
    "                                  \"Quantum Computing\", \"Bioinformatics\"],\n",
    "        \n",
    "        \"Physical Sciences\": [\"Physics\", \"Chemistry\", \"Mathematics\", \"Astronomy\", \"Astrophysics\", \n",
    "                             \"Materials Science\", \"Nanotechnology\"],\n",
    "        \n",
    "        \"Engineering\": [\"Engineering\", \"Mechanical Engineering\", \"Electrical Engineering\", \"Civil Engineering\",\n",
    "                       \"Chemical Engineering\", \"Aerospace Engineering\", \"Biomedical Engineering\",\n",
    "                       \"Environmental Engineering\", \"Industrial Engineering\", \"Nuclear Engineering\"],\n",
    "        \n",
    "        \"Earth & Environment\": [\"Earth Science\", \"Environmental Science\", \"Geology\", \"Meteorology\", \n",
    "                               \"Oceanography\", \"Climate Science\", \"Environmental Studies\", \"Sustainability\"],\n",
    "        \n",
    "        \"Social Sciences\": [\"Psychology\", \"Sociology\", \"Political Science\", \"Economics\", \"Anthropology\",\n",
    "                           \"Education\", \"Criminology\", \"Social Work\", \"International Relations\",\n",
    "                           \"Public Policy\", \"Urban Planning\", \"Communication\", \"Neuroscience\", \n",
    "                           \"Cognitive Science\"],\n",
    "        \n",
    "        \"Humanities\": [\"History\", \"Philosophy\", \"Literature\", \"Art\", \"Music\", \"Theater\", \"Linguistics\",\n",
    "                      \"Languages\", \"Cultural Studies\", \"Religious Studies\", \"Media Studies\", \"Film Studies\"],\n",
    "        \n",
    "        \"Business & Applied\": [\"Business\", \"Management\", \"Marketing\", \"Finance\", \"Accounting\",\n",
    "                              \"Operations Research\", \"Supply Chain\", \"Entrepreneurship\", \"Law\", \n",
    "                              \"Legal Studies\", \"Architecture\", \"Design\"]\n",
    "    }\n",
    "    \n",
    "    # Assign domains to categories\n",
    "    domain_to_category = {}\n",
    "    for category, domains in domain_categories.items():\n",
    "        for domain in domains:\n",
    "            if domain in G.nodes():\n",
    "                domain_to_category[domain] = category\n",
    "    \n",
    "    # Handle unassigned domains\n",
    "    for domain in G.nodes():\n",
    "        if domain not in domain_to_category:\n",
    "            domain_to_category[domain] = \"Other\"\n",
    "    \n",
    "    return domain_to_category\n",
    "\n",
    "def save_multiple_views(G, year, stats, output_dir=\"domain_graphs\"):\n",
    "    \"\"\"Create multiple clear views of the network\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if len(G.nodes()) == 0:\n",
    "        print(f\"âš  No nodes to visualize for {year}\")\n",
    "        return\n",
    "    \n",
    "    domain_to_category = create_domain_clusters(G, stats)\n",
    "    \n",
    "    # ========= VIEW 1: TOP DOMAINS BAR CHART =========\n",
    "    top_domains = stats['top_domains'][:20]\n",
    "    \n",
    "    fig_bar = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=[count for domain, count in top_domains],\n",
    "            y=[domain for domain, count in top_domains],\n",
    "            orientation='h',\n",
    "            # marker_color='viridis',\n",
    "            text=[f'{count:,}' for domain, count in top_domains],\n",
    "            textposition='auto',\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig_bar.update_layout(\n",
    "        title=f\"Top 20 Research Domains by Article Count ({year})\",\n",
    "        xaxis_title=\"Number of Articles\",\n",
    "        yaxis_title=\"Domain\",\n",
    "        height=600,\n",
    "        yaxis={'categoryorder':'total ascending'}\n",
    "    )\n",
    "    \n",
    "    fig_bar.write_html(os.path.join(output_dir, f\"top_domains_{year}.html\"))\n",
    "    \n",
    "    # ========= VIEW 2: STRONGEST COLLABORATIONS =========\n",
    "    top_collabs = stats['strongest_connections'][:15]\n",
    "    \n",
    "    collab_data = []\n",
    "    for (d1, d2), weight in top_collabs:\n",
    "        collab_data.append({\n",
    "            'Domain 1': d1,\n",
    "            'Domain 2': d2,\n",
    "            'Collaborations': weight,\n",
    "            'Pair': f\"{d1} â†” {d2}\"\n",
    "        })\n",
    "    \n",
    "    df_collab = pd.DataFrame(collab_data)\n",
    "    \n",
    "    fig_collab = go.Figure(data=[\n",
    "        go.Bar(\n",
    "            x=df_collab['Collaborations'],\n",
    "            y=df_collab['Pair'],\n",
    "            orientation='h',\n",
    "            # marker_color='plasma',\n",
    "            text=df_collab['Collaborations'],\n",
    "            textposition='auto',\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    fig_collab.update_layout(\n",
    "        title=f\"Top 15 Domain Collaborations ({year})\",\n",
    "        xaxis_title=\"Number of Collaborative Articles\",\n",
    "        yaxis_title=\"Domain Pairs\",\n",
    "        height=600,\n",
    "        yaxis={'categoryorder':'total ascending'}\n",
    "    )\n",
    "    \n",
    "    fig_collab.write_html(os.path.join(output_dir, f\"top_collaborations_{year}.html\"))\n",
    "    \n",
    "    # ========= VIEW 3: CATEGORY-BASED NETWORK =========\n",
    "    # Create a simplified network showing only category-level connections\n",
    "    category_graph = nx.Graph()\n",
    "    category_connections = defaultdict(int)\n",
    "    \n",
    "    for u, v, data in G.edges(data=True):\n",
    "        cat_u = domain_to_category.get(u, \"Other\")\n",
    "        cat_v = domain_to_category.get(v, \"Other\")\n",
    "        if cat_u != cat_v:  # Only inter-category connections\n",
    "            pair = tuple(sorted([cat_u, cat_v]))\n",
    "            category_connections[pair] += data['weight']\n",
    "    \n",
    "    for (cat1, cat2), weight in category_connections.items():\n",
    "        category_graph.add_edge(cat1, cat2, weight=weight)\n",
    "    \n",
    "    # Visualize category network\n",
    "    if len(category_graph.nodes()) > 0:\n",
    "        pos_cat = nx.spring_layout(category_graph, seed=42, k=3, iterations=100)\n",
    "        \n",
    "        edge_x = []\n",
    "        edge_y = []\n",
    "        for u, v in category_graph.edges():\n",
    "            x0, y0 = pos_cat[u]\n",
    "            x1, y1 = pos_cat[v]\n",
    "            edge_x += [x0, x1, None]\n",
    "            edge_y += [y0, y1, None]\n",
    "        \n",
    "        edge_trace = go.Scatter(\n",
    "            x=edge_x, y=edge_y,\n",
    "            line=dict(width=2, color='gray'),\n",
    "            hoverinfo='none',\n",
    "            mode='lines'\n",
    "        )\n",
    "        \n",
    "        node_x = []\n",
    "        node_y = []\n",
    "        node_text = []\n",
    "        node_size = []\n",
    "        node_info = []\n",
    "        \n",
    "        for node in category_graph.nodes():\n",
    "            x, y = pos_cat[node]\n",
    "            node_x.append(x)\n",
    "            node_y.append(y)\n",
    "            node_text.append(node)\n",
    "            \n",
    "            # Count domains in this category\n",
    "            domains_in_cat = [d for d, cat in domain_to_category.items() if cat == node]\n",
    "            node_size.append(len(domains_in_cat) * 20 + 30)\n",
    "            \n",
    "            # Hover info\n",
    "            domain_list = \"<br>\".join([f\"â€¢ {d}\" for d in domains_in_cat[:10]])\n",
    "            if len(domains_in_cat) > 10:\n",
    "                domain_list += f\"<br>... and {len(domains_in_cat)-10} more\"\n",
    "            \n",
    "            node_info.append(f\"<b>{node}</b><br>Domains: {len(domains_in_cat)}<br>{domain_list}\")\n",
    "        \n",
    "        node_trace = go.Scatter(\n",
    "            x=node_x, y=node_y,\n",
    "            mode='markers+text',\n",
    "            text=node_text,\n",
    "            textposition=\"middle center\",\n",
    "            textfont=dict(size=12, color=\"white\", family=\"Arial Black\"),\n",
    "            hovertext=node_info,\n",
    "            hoverinfo='text',\n",
    "            marker=dict(\n",
    "                size=node_size,\n",
    "                color=list(range(len(node_x))),\n",
    "                # colorscale='Set3',\n",
    "                line=dict(width=2, color='white')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig_cat = go.Figure(data=[edge_trace, node_trace],\n",
    "                           layout=go.Layout(\n",
    "                               title=f\"Research Domain Categories Network ({year})\",\n",
    "                               showlegend=False,\n",
    "                               hovermode='closest',\n",
    "                               margin=dict(b=20, l=5, r=5, t=40),\n",
    "                               xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                               yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                               width=800,\n",
    "                               height=600\n",
    "                           ))\n",
    "        \n",
    "        fig_cat.write_html(os.path.join(output_dir, f\"category_network_{year}.html\"))\n",
    "    \n",
    "    # ========= VIEW 4: FOCUSED SUBNETWORKS =========\n",
    "    # Create focused views of the most connected domains\n",
    "    top_domain_names = [domain for domain, count in stats['top_domains'][:15]]\n",
    "    subgraph = G.subgraph(top_domain_names).copy()\n",
    "    \n",
    "    if len(subgraph.edges()) > 0:\n",
    "        # Filter to strong connections only\n",
    "        edges_to_keep = [(u, v) for u, v, d in subgraph.edges(data=True) if d['weight'] >= 5]\n",
    "        focused_graph = subgraph.edge_subgraph(edges_to_keep).copy()\n",
    "        \n",
    "        if len(focused_graph.nodes()) > 0:\n",
    "            pos_focused = nx.spring_layout(focused_graph, seed=42, k=2, iterations=100)\n",
    "            \n",
    "            # Create focused network visualization\n",
    "            edge_x = []\n",
    "            edge_y = []\n",
    "            for u, v in focused_graph.edges():\n",
    "                x0, y0 = pos_focused[u]\n",
    "                x1, y1 = pos_focused[v]\n",
    "                edge_x += [x0, x1, None]\n",
    "                edge_y += [y0, y1, None]\n",
    "            \n",
    "            edge_trace = go.Scatter(\n",
    "                x=edge_x, y=edge_y,\n",
    "                line=dict(width=1, color='lightgray'),\n",
    "                hoverinfo='none',\n",
    "                mode='lines'\n",
    "            )\n",
    "            \n",
    "            node_x = []\n",
    "            node_y = []\n",
    "            node_text = []\n",
    "            node_size = []\n",
    "            node_info = []\n",
    "            \n",
    "            for node in focused_graph.nodes():\n",
    "                x, y = pos_focused[node]\n",
    "                node_x.append(x)\n",
    "                node_y.append(y)\n",
    "                \n",
    "                # Shorter labels\n",
    "                short_name = node.replace(' and ', ' & ').replace('Science', 'Sci.')\n",
    "                node_text.append(short_name)\n",
    "                \n",
    "                degree = focused_graph.degree(node)\n",
    "                node_size.append(degree * 15 + 25)\n",
    "                \n",
    "                # Hover with collaboration details\n",
    "                neighbors = []\n",
    "                for neighbor in focused_graph.neighbors(node):\n",
    "                    weight = focused_graph[node][neighbor]['weight']\n",
    "                    neighbors.append(f\"â€¢ {neighbor}: {weight}\")\n",
    "                \n",
    "                neighbor_text = \"<br>\".join(neighbors)\n",
    "                node_info.append(f\"<b>{node}</b><br>Connections: {degree}<br>{neighbor_text}\")\n",
    "            \n",
    "            node_trace = go.Scatter(\n",
    "                x=node_x, y=node_y,\n",
    "                mode='markers+text',\n",
    "                text=node_text,\n",
    "                textposition=\"middle center\",\n",
    "                textfont=dict(size=10, color=\"white\"),\n",
    "                hovertext=node_info,\n",
    "                hoverinfo='text',\n",
    "                marker=dict(\n",
    "                    size=node_size,\n",
    "                    color=[focused_graph.degree(n) for n in focused_graph.nodes()],\n",
    "                    # colorscale='Viridis',\n",
    "                    showscale=True,\n",
    "                    line=dict(width=2, color='white')\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            fig_focused = go.Figure(data=[edge_trace, node_trace],\n",
    "                                   layout=go.Layout(\n",
    "                                       title=f\"Top 15 Domains Network ({year}) - Strong Connections Only\",\n",
    "                                       showlegend=False,\n",
    "                                       hovermode='closest',\n",
    "                                       margin=dict(b=20, l=5, r=5, t=40),\n",
    "                                       xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                                       yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "                                       width=900,\n",
    "                                       height=700\n",
    "                                   ))\n",
    "            \n",
    "            fig_focused.write_html(os.path.join(output_dir, f\"focused_network_{year}.html\"))\n",
    "    \n",
    "    # Save statistics\n",
    "    with open(os.path.join(output_dir, f\"stats_{year}.json\"), \"w\") as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    \n",
    "    print(f\"ðŸ’¾ Created multiple visualizations for {year}:\")\n",
    "    print(f\"   ðŸ“Š Top domains: top_domains_{year}.html\")\n",
    "    print(f\"   ðŸ¤ Collaborations: top_collaborations_{year}.html\") \n",
    "    print(f\"   ðŸ·ï¸  Categories: category_network_{year}.html\")\n",
    "    print(f\"   ðŸŽ¯ Focused network: focused_network_{year}.html\")\n",
    "    print(f\"   ðŸ“ˆ Stats: stats_{year}.json\")\n",
    "\n",
    "def main():\n",
    "    years = [2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "    \n",
    "    for year in years:\n",
    "        print(f\"\\nðŸ”„ Processing {year}...\")\n",
    "        G, stats = build_domain_graph_fast(year)\n",
    "        \n",
    "        if G and len(G.nodes()) > 0:\n",
    "            save_multiple_views(G, year, stats)\n",
    "        else:\n",
    "            print(f\"âš  No valid graph generated for {year}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63c3cd-1651-437c-87e3-2aad627cdc32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_310)",
   "language": "python",
   "name": "env_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
