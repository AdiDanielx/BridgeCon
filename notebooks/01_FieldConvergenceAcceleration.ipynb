{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b66a3-c9ee-41e0-ade8-2095c3202efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import itertools\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "YEARS = list(range(2018, 2026))  \n",
    "INPUT_FOLDER_TEMPLATE = \"articles_{year}_new\"  \n",
    "INPUT_FILE_NAME = \"all_articles_enhanced.jsonl\"\n",
    "\n",
    "TOP_K_FIELDS_PER_YEAR = 200 \n",
    "\n",
    "OUTPUT_ROOT = \"field_convergence_A\"\n",
    "\n",
    "def read_year_data(year):\n",
    "    \"\"\"\n",
    "    Read the yearly JSONL file for a given year.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year : int\n",
    "        The publication year to load.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        List of enriched article records for that year.\n",
    "        Each record corresponds to one paper, including fields and metadata.\n",
    "    \"\"\"\n",
    "    folder = INPUT_FOLDER_TEMPLATE.format(year=year)\n",
    "    path = os.path.join(folder, INPUT_FILE_NAME)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"⚠ קובץ לא נמצא לשנה {year}: {path}\")\n",
    "        return []\n",
    "\n",
    "    items = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                items.append(obj)\n",
    "            except Exception:\n",
    "                # מסתנן שורות פגומות אם יש\n",
    "                continue\n",
    "    return items\n",
    "\n",
    "\n",
    "def extract_paper_fields(item):\n",
    "    \"\"\"\n",
    "    Extract the list of unique research fields from an article record.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    item : dict\n",
    "        Enriched article object as produced by `enhance_work_data`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str\n",
    "        Sorted unique list of field names associated with the article.\n",
    "    \"\"\"\n",
    "    fields = item.get(\"fields\") or []\n",
    "    fields = [str(x).strip() for x in fields if x and str(x).strip()]\n",
    "    return sorted(set(fields))\n",
    "\n",
    "\n",
    "def cosine_weight(co_ij, c_i, c_j, eps=1e-12):\n",
    "    \"\"\"\n",
    "    Compute cosine-normalized co-occurrence weight for a field pair.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    co_ij : int\n",
    "        Number of co-occurrences of fields i and j in the same articles.\n",
    "    c_i : int\n",
    "        Number of articles containing field i.\n",
    "    c_j : int\n",
    "        Number of articles containing field j.\n",
    "    eps : float, optional\n",
    "        Small constant for numerical stability (default=1e-12).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cosine-normalized co-occurrence weight.\n",
    "    \"\"\"\n",
    "\n",
    "    denom = math.sqrt(max(c_i, 0) * max(c_j, 0)) + eps\n",
    "    return co_ij / denom\n",
    "\n",
    "\n",
    "def linear_slope(xs, ys):\n",
    "    \"\"\"\n",
    "    Estimate the slope of a linear regression line (least squares).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xs : array-like\n",
    "        Independent variable values (e.g., years).\n",
    "    ys : array-like\n",
    "        Dependent variable values (e.g., pairwise weights).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    slope : float\n",
    "        Estimated slope of the regression line.\n",
    "    intercept : float\n",
    "        Estimated intercept of the regression line.\n",
    "    \"\"\"\n",
    "    if len(xs) < 2:\n",
    "        return float(\"nan\"), float(\"nan\")  # לא מספיק נקודות\n",
    "    slope, intercept = np.polyfit(xs, ys, 1)\n",
    "    return float(slope), float(intercept)\n",
    "\n",
    "\n",
    "def ensure_output_dir():\n",
    "    \"\"\"\n",
    "    Create a timestamped output directory under OUTPUT_ROOT.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Path to the newly created output directory.\n",
    "    \"\"\"\n",
    "\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = f\"{OUTPUT_ROOT}_{ts}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "def build_year_field_counts(year_items):\n",
    "    \"\"\"\n",
    "    Count field frequencies and field-pair co-occurrences for a given year.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year_items : list of dict\n",
    "        List of enriched article records for a given year.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    field_count : collections.Counter\n",
    "        Number of articles containing each field.\n",
    "    pair_count : collections.Counter\n",
    "        Number of articles containing each pair of fields.\n",
    "    \"\"\"\n",
    "\n",
    "    field_count = Counter()\n",
    "    pair_count = Counter()\n",
    "\n",
    "    for it in year_items:\n",
    "        fields = extract_paper_fields(it)\n",
    "        if len(fields) == 0:\n",
    "            continue\n",
    "\n",
    "        field_count.update(fields)\n",
    "\n",
    "        if len(fields) > 1:\n",
    "            for a, b in itertools.combinations(sorted(fields), 2):\n",
    "                pair_count[(a, b)] += 1\n",
    "\n",
    "    return field_count, pair_count\n",
    "\n",
    "\n",
    "def limit_to_top_k(field_count, pair_count, top_k):\n",
    "    \"\"\"\n",
    "    Filter to only Top-K most frequent fields in a given year.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    field_count : collections.Counter\n",
    "        Field frequencies for the year.\n",
    "    pair_count : collections.Counter\n",
    "        Field-pair co-occurrence frequencies for the year.\n",
    "    top_k : int or None\n",
    "        Number of top fields to keep (None = keep all).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    field_count_limited : collections.Counter\n",
    "        Filtered field counts.\n",
    "    pair_count_limited : collections.Counter\n",
    "        Filtered pair counts (only pairs with both fields in top fields).\n",
    "    \"\"\"\n",
    "\n",
    "    if top_k is None:\n",
    "        return field_count, pair_count\n",
    "\n",
    "    top_fields = set([f for f, _ in field_count.most_common(top_k)])\n",
    "\n",
    "    field_count_limited = Counter({f: c for f, c in field_count.items() if f in top_fields})\n",
    "\n",
    "    pair_count_limited = Counter({(a, b): c for (a, b), c in pair_count.items()\n",
    "                                  if a in top_fields and b in top_fields})\n",
    "    return field_count_limited, pair_count_limited\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution pipeline for Field Convergence Acceleration (FCA).\n",
    "\n",
    "    Steps:\n",
    "    ------\n",
    "    1. For each year (2018–2025):\n",
    "       - Read enriched article data.\n",
    "       - Count field and field-pair occurrences.\n",
    "       - Compute cosine-normalized weights.\n",
    "\n",
    "    2. Across years:\n",
    "       - Fit linear slopes for each field pair with ≥3 years of data.\n",
    "       - Compute FCA (slope) and related statistics.\n",
    "\n",
    "    3. Save results:\n",
    "       - `per_year_field_weights.csv`\n",
    "       - `fca_summary.csv`\n",
    "       - `top_converging_pairs.csv`\n",
    "       - `README.txt` with description.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - Output is stored in a timestamped folder under OUTPUT_ROOT.\n",
    "    - Top-K filtering controls scalability.\n",
    "    \"\"\"\n",
    "\n",
    "    out_dir = ensure_output_dir()\n",
    "\n",
    "    rows_weights = []\n",
    "\n",
    "    per_year_field_counts = {}\n",
    "    per_year_pair_counts = {}\n",
    "\n",
    "    for year in YEARS:\n",
    "        items = read_year_data(year)\n",
    "        if not items:\n",
    "            continue\n",
    "\n",
    "        field_count, pair_count = build_year_field_counts(items)\n",
    "\n",
    "        field_count, pair_count = limit_to_top_k(field_count, pair_count, TOP_K_FIELDS_PER_YEAR)\n",
    "\n",
    "        per_year_field_counts[year] = field_count\n",
    "        per_year_pair_counts[year] = pair_count\n",
    "\n",
    "        for (a, b), co_ij in pair_count.items():\n",
    "            c_i = field_count.get(a, 0)\n",
    "            c_j = field_count.get(b, 0)\n",
    "            w = cosine_weight(co_ij, c_i, c_j)\n",
    "            rows_weights.append({\n",
    "                \"year\": year,\n",
    "                \"field_i\": a,\n",
    "                \"field_j\": b,\n",
    "                \"count_i\": c_i,\n",
    "                \"count_j\": c_j,\n",
    "                \"coocc\": co_ij,\n",
    "                \"weight_cosine\": w\n",
    "            })\n",
    "\n",
    "    if not rows_weights:\n",
    "        print(\"לא נמצאו נתונים לחישוב משקלים. ודאי שהקבצים קיימים במבנה התקיות הנדרש.\")\n",
    "        return\n",
    "\n",
    "    df_weights = pd.DataFrame(rows_weights)\n",
    "\n",
    "    fca_rows = []\n",
    "    for (a, b), sub in df_weights.groupby([\"field_i\", \"field_j\"]):\n",
    "        sub = sub.sort_values(\"year\")\n",
    "        xs = sub[\"year\"].values.astype(float)\n",
    "        ys = sub[\"weight_cosine\"].values.astype(float)\n",
    "\n",
    "        if len(sub) < 3:\n",
    "            continue\n",
    "\n",
    "        slope, intercept = linear_slope(xs, ys)\n",
    "\n",
    "        fca_rows.append({\n",
    "            \"field_i\": a,\n",
    "            \"field_j\": b,\n",
    "            \"years_covered\": len(sub),\n",
    "            \"year_first\": int(sub[\"year\"].min()),\n",
    "            \"year_last\": int(sub[\"year\"].max()),\n",
    "            \"weight_first\": float(sub.iloc[0][\"weight_cosine\"]),\n",
    "            \"weight_last\": float(sub.iloc[-1][\"weight_cosine\"]),\n",
    "            \"weight_mean\": float(sub[\"weight_cosine\"].mean()),\n",
    "            \"FCA_slope\": slope,\n",
    "            \"FCA_intercept\": intercept\n",
    "        })\n",
    "\n",
    "    if not fca_rows:\n",
    "        return\n",
    "\n",
    "    df_fca = pd.DataFrame(fca_rows)\n",
    "    df_top = df_fca.sort_values(\"FCA_slope\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    path_weights = os.path.join(out_dir, \"per_year_field_weights.csv\")\n",
    "    path_fca = os.path.join(out_dir, \"fca_summary.csv\")\n",
    "    path_top = os.path.join(out_dir, \"top_converging_pairs.csv\")\n",
    "    df_weights.to_csv(path_weights, index=False, encoding=\"utf-8\")\n",
    "    df_fca.to_csv(path_fca, index=False, encoding=\"utf-8\")\n",
    "    df_top.to_csv(path_top, index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_310)",
   "language": "python",
   "name": "env_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
