{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b977f9c-23fa-4cc2-88d8-3687ccd9431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import glob\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional deps\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    import statsmodels.formula.api as smf\n",
    "except Exception:\n",
    "    sm = None\n",
    "    smf = None\n",
    "\n",
    "try:\n",
    "    from scipy import stats\n",
    "except Exception:\n",
    "    stats = None\n",
    "\n",
    "YEARS = list(range(2018, 2026))            \n",
    "ARTICLES_DIR_TEMPLATE = \"articles_{year}_new\"\n",
    "ARTICLES_FILE = \"all_articles_enhanced.jsonl\"\n",
    "\n",
    "TOP_N_PAIRS = 50          \n",
    "BRIDGE_QUANTILE = 0.90   \n",
    "MIN_YEARS_PER_PAIR = 3   \n",
    "N_PERM = 3000             \n",
    "\n",
    "def find_latest_dir(prefix: str):\n",
    "    cands = [d for d in os.listdir(\".\") if d.startswith(prefix) and os.path.isdir(d)]\n",
    "    if not cands: return None\n",
    "    cands.sort(reverse=True)\n",
    "    return cands[0]\n",
    "\n",
    "def ensure_outdirs():\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    root = f\"paper_ready_{ts}\"\n",
    "    figdir = os.path.join(root, \"figures\")\n",
    "    tbldir = os.path.join(root, \"tables\")\n",
    "    os.makedirs(figdir, exist_ok=True)\n",
    "    os.makedirs(tbldir, exist_ok=True)\n",
    "    return root, figdir, tbldir\n",
    "\n",
    "def load_partA():\n",
    "    a_dir = find_latest_dir(\"field_convergence_A_\")\n",
    "    if not a_dir: raise FileNotFoundError(\"Missing field_convergence_A_* folder.\")\n",
    "    fca_path = os.path.join(a_dir, \"fca_summary.csv\")\n",
    "    w_year_path = os.path.join(a_dir, \"per_year_field_weights.csv\")\n",
    "    if not os.path.exists(fca_path): raise FileNotFoundError(f\"Missing {fca_path}\")\n",
    "    if not os.path.exists(w_year_path): raise FileNotFoundError(f\"Missing {w_year_path}\")\n",
    "    df_fca = pd.read_csv(fca_path)\n",
    "    df_wy = pd.read_csv(w_year_path)\n",
    "    df_fca[\"pair\"] = list(map(lambda xy: tuple(sorted((xy[0], xy[1]))),\n",
    "                              zip(df_fca[\"field_i\"], df_fca[\"field_j\"])))\n",
    "    df_wy[\"pair\"] = list(map(lambda xy: tuple(sorted((xy[0], xy[1]))),\n",
    "                             zip(df_wy[\"field_i\"], df_wy[\"field_j\"])))\n",
    "    return a_dir, df_fca, df_wy\n",
    "\n",
    "def load_partB():\n",
    "    b_dir = find_latest_dir(\"bridge_emergence_B_\")\n",
    "    if not b_dir: raise FileNotFoundError(\"Missing bridge_emergence_B_* folder.\")\n",
    "    bes_path = os.path.join(b_dir, \"bes_summary.csv\")\n",
    "    per_year_part_path = os.path.join(b_dir, \"per_year_author_participation.csv\")\n",
    "    if not os.path.exists(bes_path): raise FileNotFoundError(f\"Missing {bes_path}\")\n",
    "    if not os.path.exists(per_year_part_path): raise FileNotFoundError(f\"Missing {per_year_part_path}\")\n",
    "    df_bes = pd.read_csv(bes_path)\n",
    "    df_bes[\"author_id\"] = df_bes[\"author_id\"].astype(str)\n",
    "    return b_dir, df_bes\n",
    "\n",
    "def load_partC():\n",
    "    c_dir = find_latest_dir(\"novelty_AB_\")\n",
    "    # Part C is helpful for CPR aggregations and scatter/box figures (optional)\n",
    "    # If not present, we will recompute what we need\n",
    "    return c_dir\n",
    "\n",
    "def copy_if_exists(src, dst):\n",
    "    try:\n",
    "        if src and os.path.exists(src):\n",
    "            import shutil\n",
    "            shutil.copy2(src, dst)\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "def top_bottom_pairs(df_fca, n=TOP_N_PAIRS):\n",
    "    df_fca = df_fca[df_fca[\"years_covered\"] >= MIN_YEARS_PER_PAIR].copy()\n",
    "    df_sorted = df_fca.sort_values(\"FCA_slope\", ascending=False)\n",
    "    top = df_sorted.head(n).copy()\n",
    "    bottom = df_sorted.tail(n).copy()\n",
    "    return top, bottom\n",
    "\n",
    "def permutation_test_spearman(x, y, n_perm=3000, seed=7):\n",
    "    \"\"\"\n",
    "    Two-sided permutation test for Spearman correlation.\n",
    "    Returns corr_obs, p_perm\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, float)\n",
    "    y = np.asarray(y, float)\n",
    "    mask = np.isfinite(x) & np.isfinite(y)\n",
    "    x = x[mask]; y = y[mask]\n",
    "    if len(x) < 5:\n",
    "        return np.nan, np.nan\n",
    "    if stats is None:\n",
    "        # Fallback: no permutation\n",
    "        corr = pd.Series(x).corr(pd.Series(y), method=\"spearman\")\n",
    "        return float(corr), np.nan\n",
    "    corr_obs, _ = stats.spearmanr(x, y)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    more_extreme = 0\n",
    "    for _ in range(n_perm):\n",
    "        y_perm = rng.permutation(y)\n",
    "        c, _ = stats.spearmanr(x, y_perm)\n",
    "        if abs(c) >= abs(corr_obs):\n",
    "            more_extreme += 1\n",
    "    p = (more_extreme + 1) / (n_perm + 1)\n",
    "    return float(corr_obs), float(p)\n",
    "\n",
    "def mannwhitney_and_cliffs_delta(a, b):\n",
    "    \"\"\"\n",
    "    Returns MW U p-value and Cliff's delta.\n",
    "    \"\"\"\n",
    "    a = np.asarray([v for v in a if np.isfinite(v)], float)\n",
    "    b = np.asarray([v for v in b if np.isfinite(v)], float)\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return np.nan, np.nan\n",
    "    if stats is None:\n",
    "        return np.nan, np.nan\n",
    "    u_stat, p = stats.mannwhitneyu(a, b, alternative=\"two-sided\")\n",
    "    # Cliff's delta\n",
    "    # Efficient approximation using ranks:\n",
    "    # delta = (2*U/(n_a*n_b)) - 1\n",
    "    delta = (2 * u_stat / (len(a)*len(b))) - 1\n",
    "    return float(p), float(delta)\n",
    "\n",
    "def plot_pair_trends(df_wy, pair, figdir):\n",
    "    \"\"\"\n",
    "    Draws a line chart of weight_cosine over years for a given field pair.\n",
    "    Saves to figures/.\n",
    "    \"\"\"\n",
    "    sub = df_wy[(df_wy[\"pair\"] == pair) & (df_wy[\"year\"].isin(YEARS))].copy()\n",
    "    if sub.empty:\n",
    "        return None\n",
    "    sub = sub.groupby(\"year\", as_index=False)[\"weight_cosine\"].mean().sort_values(\"year\")\n",
    "    plt.figure(figsize=(6.2,4.2))\n",
    "    plt.plot(sub[\"year\"], sub[\"weight_cosine\"], marker=\"o\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Cosine weight (co-occurrence)\")\n",
    "    plt.title(f\"Trend: {pair[0]} × {pair[1]}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    fname = f\"pair_trends_field_weights_{pair[0].replace('/','-')}__{pair[1].replace('/','-')}.png\"\n",
    "    path = os.path.join(figdir, fname)\n",
    "    plt.savefig(path, dpi=140)\n",
    "    plt.close()\n",
    "    return path\n",
    "\n",
    "def main():\n",
    "    root, figdir, tbldir = ensure_outdirs()\n",
    "\n",
    "    # Load A/B/C\n",
    "    a_dir, df_fca, df_wy = load_partA()\n",
    "    b_dir, df_bes = load_partB()\n",
    "    c_dir = load_partC()\n",
    "\n",
    "    # Load link file from C if exists (contains bridge shares & CPR per pair)\n",
    "    df_link = None\n",
    "    if c_dir:\n",
    "        link_path = os.path.join(c_dir, \"link_fca_bes_per_pair.csv\")\n",
    "        if os.path.exists(link_path):\n",
    "            df_link = pd.read_csv(link_path)\n",
    "            # ensure numeric\n",
    "            for col in [\"FCA_slope\", \"share_bridge_authors\", \"cpr_mean\", \"cpr_median\"]:\n",
    "                if col in df_link.columns:\n",
    "                    df_link[col] = pd.to_numeric(df_link[col], errors=\"coerce\")\n",
    "\n",
    "    # 1) Correlation (FCA vs Bridge share) + permutation p-value\n",
    "    corr_rows = []\n",
    "    if df_link is not None and \"share_bridge_authors\" in df_link.columns:\n",
    "        corr_obs, p_perm = permutation_test_spearman(df_link[\"FCA_slope\"], df_link[\"share_bridge_authors\"], n_perm=N_PERM)\n",
    "        corr_rows.append({\"metric_x\":\"FCA_slope\",\"metric_y\":\"share_bridge_authors\",\"spearman\":corr_obs,\"p_perm\":p_perm})\n",
    "    else:\n",
    "        # fallback: compute from scratch minimally (share not trivial → skip)\n",
    "        corr_rows.append({\"metric_x\":\"FCA_slope\",\"metric_y\":\"share_bridge_authors\",\"spearman\":np.nan,\"p_perm\":np.nan})\n",
    "    pd.DataFrame(corr_rows).to_csv(os.path.join(tbldir,\"correlation_summary.csv\"), index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 2) GLM: share_bridge_authors ~ FCA_slope + weight_mean + years_covered (with weights = authors_total)\n",
    "    if sm is not None and smf is not None and df_link is not None and {\"authors_total\",\"authors_bridge\",\"FCA_slope\",\"weight_mean\",\"year_first\",\"year_last\"}.issubset(df_link.columns):\n",
    "        # Construct response as proportion with weights\n",
    "        df_glm = df_link.copy()\n",
    "        df_glm[\"share\"] = df_glm[\"authors_bridge\"] / df_glm[\"authors_total\"].replace(0, np.nan)\n",
    "        df_glm[\"years_range\"] = df_glm[\"year_last\"] - df_glm[\"year_first\"] + 1\n",
    "        df_glm = df_glm.replace([np.inf,-np.inf], np.nan).dropna(subset=[\"share\",\"FCA_slope\",\"weight_mean\",\"years_range\",\"authors_total\"])\n",
    "        if len(df_glm) >= 10:\n",
    "            # Binomial GLM with weights (number of trials)\n",
    "            # Use quasi-binomial approximation via GLM Binomial + robust SE\n",
    "            endog = pd.DataFrame({\"success\": df_glm[\"authors_bridge\"], \"failure\": df_glm[\"authors_total\"] - df_glm[\"authors_bridge\"]})\n",
    "            df_glm[\"success\"] = df_glm[\"authors_bridge\"]\n",
    "            df_glm[\"total\"]   = df_glm[\"authors_total\"]\n",
    "            formula = \"success ~ FCA_slope + weight_mean + years_range\"\n",
    "            model = smf.glm(formula=formula, data=df_glm, family=sm.families.Binomial(), freq_weights=df_glm[\"total\"])\n",
    "            res = model.fit(cov_type=\"HC3\")\n",
    "            summ = res.summary2().tables[1].reset_index().rename(columns={\"index\":\"term\"})\n",
    "            summ.to_csv(os.path.join(tbldir,\"glm_bridge_share_logit.csv\"), index=False, encoding=\"utf-8\")\n",
    "            try:\n",
    "                with open(os.path.join(tbldir,\"glm_bridge_share_logit.tex\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(summ.to_latex(index=False, float_format=\"%.4f\"))\n",
    "            except Exception:\n",
    "                pass\n",
    "        else:\n",
    "            pd.DataFrame({\"note\":[\"Not enough rows for GLM\"]}).to_csv(os.path.join(tbldir,\"glm_bridge_share_logit.csv\"), index=False, encoding=\"utf-8\")\n",
    "    else:\n",
    "        pd.DataFrame({\"note\":[\"statsmodels not available or link file missing\"]}).to_csv(os.path.join(tbldir,\"glm_bridge_share_logit.csv\"), index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 3) Impact tests (Top vs Bottom FCA) using CPR\n",
    "    impact_rows = []\n",
    "    if df_link is not None and \"cpr_mean\" in df_link.columns:\n",
    "        df_sorted = df_link.sort_values(\"FCA_slope\", ascending=False)\n",
    "        df_top = df_sorted.head(TOP_N_PAIRS)\n",
    "        df_bot = df_sorted.tail(TOP_N_PAIRS)\n",
    "        # Use per-paper CPR aggregated in Part C (pair_paper_cpr collapsed). Here we use pair-level means as proxy.\n",
    "        a = df_top[\"cpr_mean\"].values\n",
    "        b = df_bot[\"cpr_mean\"].values\n",
    "        # Mann-Whitney + Cliff's delta\n",
    "        p_mw, cliffs = mannwhitney_and_cliffs_delta(a, b)\n",
    "        impact_rows.append({\"n_top\":len(a), \"n_bottom\":len(b),\n",
    "                            \"mean_top\":np.nanmean(a), \"mean_bottom\":np.nanmean(b),\n",
    "                            \"delta_mean\":np.nanmean(a)-np.nanmean(b),\n",
    "                            \"mannwhitney_p\":p_mw, \"cliffs_delta\":cliffs})\n",
    "    else:\n",
    "        impact_rows.append({\"note\":\"No CPR in link file; run Part C first.\"})\n",
    "    pd.DataFrame(impact_rows).to_csv(os.path.join(tbldir,\"impact_tests.csv\"), index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 4) Figures: copy from C if exists, else redraw scatter/box quickly from df_link\n",
    "    # scatter\n",
    "    scatter_src = os.path.join(c_dir, \"plots\",\"scatter_fca_vs_bridge_share.png\") if c_dir else None\n",
    "    if not copy_if_exists(scatter_src, os.path.join(figdir,\"scatter_fca_vs_bridge_share.png\")) and df_link is not None:\n",
    "        plt.figure(figsize=(7,5))\n",
    "        plt.scatter(df_link[\"FCA_slope\"], df_link[\"share_bridge_authors\"])\n",
    "        plt.xlabel(\"FCA_slope (field convergence acceleration)\")\n",
    "        plt.ylabel(\"Share of bridge authors\")\n",
    "        plt.title(\"FCA vs Bridge share\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(figdir,\"scatter_fca_vs_bridge_share.png\"), dpi=140)\n",
    "        plt.close()\n",
    "\n",
    "    # box\n",
    "    box_src = os.path.join(c_dir, \"plots\",\"box_impact_top_vs_bottom.png\") if c_dir else None\n",
    "    copy_if_exists(box_src, os.path.join(figdir,\"box_impact_top_vs_bottom.png\"))\n",
    "    vio_src = os.path.join(c_dir, \"plots\",\"violin_impact_top_vs_bottom.png\") if c_dir else None\n",
    "    copy_if_exists(vio_src, os.path.join(figdir,\"violin_impact_top_vs_bottom.png\"))\n",
    "\n",
    "    # 5) Field-pair trends: w_ij(t) lines for Top 10 FCA\n",
    "    top10, _bottom = top_bottom_pairs(df_fca, n=10)\n",
    "    # Table of top pairs (LaTeX)\n",
    "    try:\n",
    "        top_tbl = top10[[\"field_i\",\"field_j\",\"FCA_slope\",\"weight_first\",\"weight_last\",\"years_covered\"]].copy()\n",
    "        top_tbl.columns = [\"Field i\",\"Field j\",\"FCA slope\",\"Weight (first)\",\"Weight (last)\",\"Years\"]\n",
    "        with open(os.path.join(tbldir,\"top_fca_pairs.tex\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(top_tbl.to_latex(index=False, float_format=\"%.4f\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Plots\n",
    "    for _, r in top10.iterrows():\n",
    "        pair = tuple(sorted((r[\"field_i\"], r[\"field_j\"])))\n",
    "        plot_pair_trends(df_wy, pair, figdir)\n",
    "\n",
    "    # 6) Findings (short narrative)\n",
    "    findings = []\n",
    "    # correlation\n",
    "    try:\n",
    "        corr_df = pd.read_csv(os.path.join(tbldir,\"correlation_summary.csv\"))\n",
    "        spearman = corr_df.loc[0,\"spearman\"]\n",
    "        p_perm   = corr_df.loc[0,\"p_perm\"]\n",
    "        findings.append(f\"[Correlation] Spearman(FCA, Bridge Share) = {spearman:.3f} (perm p={p_perm:.4f})\")\n",
    "    except Exception:\n",
    "        findings.append(\"[Correlation] Not available.\")\n",
    "    # glm\n",
    "    if os.path.exists(os.path.join(tbldir,\"glm_bridge_share_logit.csv\")):\n",
    "        glm_df = pd.read_csv(os.path.join(tbldir,\"glm_bridge_share_logit.csv\"))\n",
    "        if \"term\" in glm_df.columns:\n",
    "            row = glm_df[glm_df[\"term\"]==\"FCA_slope\"]\n",
    "            if not row.empty and \"Coef.\" in glm_df.columns:\n",
    "                coef = float(row[\"Coef.\"].values[0])\n",
    "                findings.append(f\"[GLM] FCA_slope coefficient (logit) ≈ {coef:.3f} (robust SE in table).\")\n",
    "    # impact\n",
    "    try:\n",
    "        imp_df = pd.read_csv(os.path.join(tbldir,\"impact_tests.csv\"))\n",
    "        if \"delta_mean\" in imp_df.columns:\n",
    "            dm = float(imp_df[\"delta_mean\"].values[0])\n",
    "            p  = float(imp_df[\"mannwhitney_p\"].values[0]) if \"mannwhitney_p\" in imp_df.columns else np.nan\n",
    "            cd = float(imp_df[\"cliffs_delta\"].values[0]) if \"cliffs_delta\" in imp_df.columns else np.nan\n",
    "            findings.append(f\"[Impact] ΔCPR(Top-Bottom) = {dm:.4f}, MW p={p:.4f}, Cliff's δ={cd:.3f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    with open(os.path.join(root,\"findings.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(findings))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_310)",
   "language": "python",
   "name": "env_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
